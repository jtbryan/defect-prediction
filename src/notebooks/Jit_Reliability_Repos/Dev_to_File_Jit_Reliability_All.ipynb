{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis of Vagrant Developers\n",
    "<a href=\"#TODO's\">TODO</a><br>\n",
    "<a href=\"#Imports\">Imports</a><br>\n",
    "<a href=\"#Functions\">Functions</a><br>\n",
    "<a href=\"#Analysis\">Analysis</a><br>\n",
    "<a href=\"#Graph-Based-Analysis-using-Logistic-Regression,-Random-Forest-Classifer,-and-XGBoost-classifier\"><b>Analysis</b> - Graph-Based Analysis using Logistic Regression, Random Forest Classifer, and XGBoost classifier</a><br>\n",
    "<a href=\"#Cross-Validation\"><b>Analysis</b> - Cross Validation</a><br>\n",
    "<a href=\"#Rebalancing-data\"><b>Analysis</b> - Data Rebalancing</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO's\n",
    "\n",
    "<ul>\n",
    "<li>Implement a new dataframe to store the results from each section. (refer to last cell)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import json\n",
    "import csv\n",
    "import numpy\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, accuracy_score, precision_recall_curve, plot_precision_recall_curve, auc, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, LeaveOneOut, KFold, StratifiedKFold, RepeatedKFold, TimeSeriesSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "import statsmodels.api as sm\n",
    "from ast import literal_eval\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(model, x, y):\n",
    "    '''\n",
    "    Plots the predictions made using a linear regression model \n",
    "    given the set of dependent variable(s) and the independent variable\n",
    "        model: Linear Regression Model\n",
    "        x: Dependent Variable(s)\n",
    "        y: Independent Variable\n",
    "    returns: Independent Variable Predictions\n",
    "    '''\n",
    "    y_pred = model.predict(x)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_pred, color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# source: https://stackoverflow.com/questions/26319259/how-to-get-a-regression-summary-in-python-scikit-like-r-does\n",
    "def regression_results(y_true, y_pred):\n",
    "    '''\n",
    "    Analyzes the results from the linear regression model prediction using different metrics, such r^2\n",
    "    '''\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    #mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    #print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "def Loo(model, x, y):\n",
    "    '''\n",
    "    Uses the LeaveOneOut cross-validation method provided by SkLearn\n",
    "    '''\n",
    "    loo = LeaveOneOut() \n",
    "    highestscore = (0, \"\")\n",
    "    y_true, y_pred = list(), list()\n",
    "    \n",
    "    # Split the data\n",
    "    for train_index, test_index in loo.split(x):\n",
    "        x_train, x_test = x.loc[train_index], x.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        \n",
    "        # fit the model on the new data\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        #evaluate model\n",
    "        predictions = model.predict_proba(x_test)\n",
    "        yhat = model.predict(x_test) \n",
    "        \n",
    "        # determine PRC_AUC score\n",
    "        score = model.score(x_test, y_test) # NOTE: Removed the following due to errors: prc_val = average_precision_score(y_test, yhat)#predictions[:,1])\n",
    "        if score > highestscore[0]:\n",
    "            highestscore = (model.score(x_test, y_test), f\"TRAIN: {train_index} | TEST: {test_index}\")\n",
    "\n",
    "        #y_true.append(y_test[0])\n",
    "        #y_pred.append(yhat[0])\n",
    "            \n",
    "    print(highestscore[1])\n",
    "    print(\"\\nModel Score: {}\\n\".format(highestscore[0]))\n",
    "    #acc = accuracy_score(y_true, y_pred)\n",
    "    #print('Accuracy: %.3f' % acc)\n",
    "    \n",
    "    \n",
    "def Loo_short(model, x, y):\n",
    "    '''\n",
    "    Uses the shortened version of the LeaveOneOut cross-validation method provided by SkLearn by using cross_val_score\n",
    "    '''\n",
    "    cv = LeaveOneOut()\n",
    "    # to see list of scoring methods, go to: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    scores = cross_val_score(model, x, y, scoring='average_precision', cv=cv)\n",
    "    print(\"Mean Average-Precision Recall Score: {}\".format(mean(scores)))\n",
    "    \n",
    "def Rkf(model, x, y, threshold=None):\n",
    "    '''\n",
    "    Uses the RepeatedKFold cross-validation method provided by SkLearn\n",
    "    '''\n",
    "    kf = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42) \n",
    "    #kf.get_n_splits(x)\n",
    "    #print(kf)\n",
    "    highestscore = (0, 0, \"\")\n",
    "    predictions = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    yhat = None\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train, x_test = x.loc[x.index.intersection(train_index)], x.loc[x.index.intersection(test_index)]\n",
    "        y_train, y_test = y.loc[y.index.intersection(train_index)], y.loc[y.index.intersection(test_index)]\n",
    "        \n",
    "        # if any dataset is empty\n",
    "        if x_train.empty or x_test.empty or y_train.empty or y_test.empty:\n",
    "            continue\n",
    "        \n",
    "        # if there is only one value (i.e. only 1's or only 0's)\n",
    "        if(len(set(y_train.values.tolist())) <= 1):\n",
    "            continue\n",
    "        \n",
    "        # fit the model on the new data\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        #evaluate model\n",
    "        if threshold is not None:\n",
    "            predictions = (model.predict_proba(x_test)[:,1] >= threshold).astype(int)\n",
    "            yhat = (model.predict_proba(x_test)[:,1] >= threshold).astype(int)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "        else:\n",
    "            predictions = model.predict_proba(x_test)\n",
    "            # uses default threshold\n",
    "            yhat = model.predict(x_test)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, predictions[:, 1])\n",
    "        \n",
    "        fscore = (2 * (np.array(precision, dtype=float) * np.array(recall, dtype=float)) / (np.array(precision, dtype=float) + np.array(recall, dtype=float)))\n",
    "        fscore[np.isnan(fscore)] = 0 \n",
    "        # locate the index of the largest f score\n",
    "        ix = np.argmax(fscore)\n",
    "        \n",
    "        yhat = model.predict(x_test) \n",
    "        \n",
    "        # Get the auc up to the best threshold point\n",
    "        pr_auc = auc(recall[ix:], precision[ix:])\n",
    "        \n",
    "        # determine PRC_AUC score\n",
    "        prc_val = average_precision_score(y_test, yhat)#predictions[:,1])\n",
    "        if prc_val > highestscore[0]:\n",
    "            highestscore = (prc_val, model.score(x_test, y_test), f\"TRAIN: {train_index} | TEST: {test_index}\", y_test, yhat, pr_auc, predictions)\n",
    "\n",
    "        #y_true.append(y_test[0])\n",
    "        #y_pred.append(yhat[0])\n",
    "            \n",
    "    print(highestscore[2])\n",
    "    print(\"\\nModel Score: {}\".format(highestscore[1]))\n",
    "    print(\"Average Precision-Recall Score: {}\".format(highestscore[0]))\n",
    "    print(\"PRC-AUC Score: {}\".format(highestscore[5]))\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(highestscore[3], highestscore[4]))\n",
    "    acc = accuracy_score(highestscore[3], highestscore[4])\n",
    "    print('Accuracy: %.3f' % acc)\n",
    "\n",
    "    # Return model score, average precision score, y_test, PRC-AUC, and Predictions\n",
    "    return highestscore[1], acc, highestscore[0], highestscore[3], highestscore[5], highestscore[6]\n",
    "    \n",
    "def Rkf_short(model, x, y):    \n",
    "    '''\n",
    "    Uses the shortened version of the RepeatedKFold cross-validation method provided by SkLearn by using cross_val_score\n",
    "    '''\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    scores = cross_val_score(model, x, y, scoring='average_precision', cv=cv)\n",
    "    print(\"Mean Average-Precision Recall Score: {}\".format(mean(scores)))\n",
    "    \n",
    "def Skf(model, x, y, threshold = None):\n",
    "    '''\n",
    "    Uses the StratifiedKFold cross-validation method provided by SkLearn\n",
    "    '''\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=None)\n",
    "    highestscore = (0, 0, \"\")\n",
    "    predictions = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    yhat = None\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        x_train, x_test = x.loc[x.index.intersection(train_index)], x.loc[x.index.intersection(test_index)]\n",
    "        y_train, y_test = y.loc[y.index.intersection(train_index)], y.loc[y.index.intersection(test_index)]\n",
    "        \n",
    "        # if any dataset is empty\n",
    "        if x_train.empty or x_test.empty or y_train.empty or y_test.empty:\n",
    "            continue\n",
    "        \n",
    "        # if there is only one value (i.e. only 1's or only 0's)\n",
    "        if(len(set(y_train.values.tolist())) <= 1):\n",
    "            continue\n",
    "        \n",
    "        # fit the model on the new data\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        #evaluate model\n",
    "        if threshold is not None:\n",
    "            predictions = (model.predict_proba(x_test)[:,1] >= threshold).astype(int)\n",
    "            yhat = (model.predict_proba(x_test)[:,1] >= threshold).astype(int)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "        else:\n",
    "            predictions = model.predict_proba(x_test)\n",
    "            # uses default threshold\n",
    "            yhat = model.predict(x_test)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, predictions[:, 1])\n",
    "            \n",
    "        fscore = (2 * (np.array(precision, dtype=float) * np.array(recall, dtype=float)) / (np.array(precision, dtype=float) + np.array(recall, dtype=float)))\n",
    "        fscore[np.isnan(fscore)] = 0 \n",
    "        # locate the index of the largest f score\n",
    "        ix = np.argmax(fscore)\n",
    "        \n",
    "        yhat = model.predict(x_test) \n",
    "        \n",
    "        # Get the auc up to the best threshold point\n",
    "        pr_auc = auc(recall[ix:], precision[ix:])\n",
    "        # determine PRC_AUC score\n",
    "        prc_val = average_precision_score(y_test, yhat)#predictions[:,1])\n",
    "        if prc_val > highestscore[0]:\n",
    "            highestscore = (prc_val, model.score(x_test, y_test), f\"TRAIN: {train_index} | TEST: {test_index}\", y_test, yhat, pr_auc, predictions)\n",
    "\n",
    "        #y_true.append(y_test[0])\n",
    "        #y_pred.append(yhat[0])\n",
    "            \n",
    "    print(highestscore[2])\n",
    "    print(\"\\nModel Score: {}\".format(highestscore[1]))\n",
    "    print(\"\\nAverage Precision-Recall Score: {}\".format(highestscore[0]))\n",
    "    print(\"PRC-AUC Score: {}\".format(highestscore[5]))\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(highestscore[3], highestscore[4]))\n",
    "    acc = accuracy_score(highestscore[3], highestscore[4])\n",
    "    print('Accuracy: %.3f' % acc)\n",
    "    \n",
    "    # Return model score, average precision score, y_test, PRC-AUC, and Predictions\n",
    "    return highestscore[1], acc, highestscore[0], highestscore[3], highestscore[5], highestscore[6]\n",
    "    \n",
    "def Skf_short(model, x, y):\n",
    "    '''\n",
    "    Uses the shortened version of the StratifiedKFold cross-validation method provided by SkLearn by using cross_val_score\n",
    "    '''\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=None)\n",
    "    scores = cross_val_score(model, x, y, scoring='average_precision', cv=cv)\n",
    "    print(\"Mean Average-Precision Recall Score: {}\".format(mean(scores)))    \n",
    "\n",
    "def Tss(model, x, y, threshold=None):\n",
    "    '''\n",
    "    Uses the TimeSeriesSplit cross-validation method provided by SkLearn\n",
    "    '''\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    highestscore = (0, 0, \"\")\n",
    "    predictions = None\n",
    "    precision = None\n",
    "    rcall = None\n",
    "    yhat = None\n",
    "    \n",
    "    for train_index, test_index in tscv.split(x):\n",
    "        x_train, x_test = x.loc[x.index.intersection(train_index)], x.loc[x.index.intersection(test_index)]\n",
    "        y_train, y_test = y.loc[y.index.intersection(train_index)], y.loc[y.index.intersection(test_index)]\n",
    "        \n",
    "        # if any dataset is empty\n",
    "        if x_train.empty or x_test.empty or y_train.empty or y_test.empty:\n",
    "            continue\n",
    "        \n",
    "        # if there is only one value (i.e. only 1's or only 0's)\n",
    "        if(len(set(y_train.values.tolist())) <= 1):\n",
    "            continue\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        #evaluate model\n",
    "        if threshold is not None:\n",
    "            predictions = (model.predict_proba(x_test)[:,1] >= threshold).astype(int)\n",
    "            yhat = (model.predict_proba(x_test)[:,1] >= threshold).astype(int)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "        else:\n",
    "            predictions = model.predict_proba(x_test)\n",
    "            # uses default threshold\n",
    "            yhat = model.predict(x_test)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, predictions[:, 1])\n",
    "        \n",
    "        fscore = (2 * (np.array(precision, dtype=float) * np.array(recall, dtype=float)) / (np.array(precision, dtype=float) + np.array(recall, dtype=float)))\n",
    "        fscore[np.isnan(fscore)] = 0 \n",
    "        # locate the index of the largest f score\n",
    "        ix = np.argmax(fscore)\n",
    "        \n",
    "        yhat = model.predict(x_test) \n",
    "        \n",
    "        # Get the auc up to the best threshold point\n",
    "        pr_auc = auc(recall[ix:], precision[ix:])\n",
    "\n",
    "        # determine PRC_AUC score\n",
    "        prc_val = average_precision_score(y_test, yhat)#predictions[:,1])\n",
    "        if prc_val > highestscore[0]:\n",
    "            highestscore = (prc_val, model.score(x_test, y_test), f\"TRAIN: {train_index} | TEST: {test_index}\", y_test, yhat, pr_auc, predictions)\n",
    "\n",
    "        #y_true.append(y_test[0])\n",
    "        #y_pred.append(yhat[0])\n",
    "            \n",
    "    print(highestscore[2])\n",
    "    print(\"\\nModel Score: {}\".format(highestscore[1]))\n",
    "    print(\"\\nAverage Precision-Recall Score: {}\".format(highestscore[0]))\n",
    "    print(\"PRC-AUC Score: {}\".format(highestscore[5]))\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(highestscore[3], highestscore[4]))\n",
    "    acc = accuracy_score(highestscore[3], highestscore[4])\n",
    "    print('Accuracy: %.3f' % acc)\n",
    "    \n",
    "    # Return model score, average precision score, y_test, PRC-AUC, and Predictions\n",
    "    return highestscore[1], acc, highestscore[0], highestscore[3], highestscore[5], highestscore[6]\n",
    "\n",
    "def Tss_short(model, x, y):\n",
    "    '''\n",
    "    Uses the shortened version of the TimeSeriesSplit cross-validation method provided by SkLearn by using cross_val_score\n",
    "    '''\n",
    "    cv = TimeSeriesSplit(n_splits=10)\n",
    "    scores = cross_val_score(model, x, y, scoring='average_precision', cv=cv)\n",
    "    print(\"Mean Average-Precision Recall Score: {}\".format(mean(scores))) \n",
    "    \n",
    "def Compare_Model_Scores(test_x1, test_x2, y_test, predictions1, predictions2, prediction_probs1, prediction_probs2, model1, model2):\n",
    "    '''\n",
    "    This method provides different metrics about the predictions associated with an independent test variable.\n",
    "    These metrics include: PRC-AUC scores, ROC-AUC scores, and the classification report provided by sklearn\n",
    "    \n",
    "    print(\"Predictions for model 1: \")\n",
    "    print(prediction_probs1)\n",
    "    print(\"\\nPredictions for model 2: \")\n",
    "    print(prediction_probs2)\n",
    "    '''\n",
    "    \n",
    "    #recall1, recall2, precision1, precision2, thresholds_list = get_precision_recall(test_x1, test_x2, y_test, model1, model2)\n",
    "    \n",
    "    # ovr: One-vs-rest\n",
    "    # ovo: One-vs-one\n",
    "    print(\"\\nScores for model 1\")\n",
    "    print(\"------------------\")\n",
    "    # Temporarily removed to retrieve precision & recall by hand\n",
    "    \n",
    "    precision1, recall1, thresholds1 = precision_recall_curve(y_test, prediction_probs1[:, 1]) \n",
    "    #retrieve probability of being 1(in second column of probs_y)\n",
    "    \n",
    "    pr_auc1 = auc(recall1, precision1)\n",
    "    roc_val1 = roc_auc_score(y_test, prediction_probs1[:, 1], multi_class='ovr')\n",
    "    print('Roc_Auc Score: {}'.format(roc_val1))\n",
    "    prc_val1 = average_precision_score(y_test, prediction_probs1[:, 1])\n",
    "    print(\"Average Precision-Recall Score: {}\".format(prc_val1))\n",
    "    print(f\"PRC-AUC for model 1: {pr_auc1}\")\n",
    "    acc1 = accuracy_score(y_test, predictions1)\n",
    "    print('Accuracy: %.3f' % acc1)\n",
    "\n",
    "    '''\n",
    "    Classification Report breakdown from https://datascience.stackexchange.com/questions/64441/how-to-interpret-classification-report-of-scikit-learn:\n",
    "    The recall means \"how many of this class you find over the whole number of element of this class\"\n",
    "\n",
    "    The precision will be \"how many are correctly classified among that class\"\n",
    "\n",
    "    The f1-score is the harmonic mean between precision & recall\n",
    "\n",
    "    The support is the number of occurence of the given class in your dataset (so you have 37.5K of class 0 and 37.5K of class 1, which is a really well balanced dataset.\n",
    "    '''\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_test, predictions1))\n",
    "\n",
    "    print(\"\\nScores for model 2\")\n",
    "    print(\"------------------\")\n",
    "    \n",
    "    # Temporarily removed to retrieve precision & recall by hand\n",
    "    precision2, recall2, thresholds2 = precision_recall_curve(y_test, prediction_probs2[:, 1])\n",
    "    \n",
    "    pr_auc2 = auc(recall2, precision2)\n",
    "    roc_val2 = roc_auc_score(y_test, prediction_probs2[:, 1], multi_class='ovr')\n",
    "    print('Roc_Auc Score: {}'.format(roc_val2))\n",
    "    prc_val2 = average_precision_score(y_test, prediction_probs2[:, 1])\n",
    "    print(\"Average Precision-Recall Score: {}\".format(prc_val2))\n",
    "    print(f\"PRC-AUC for model 2: {pr_auc2}\")\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_test, predictions2))\n",
    "    acc2 = accuracy_score(y_test, predictions2)\n",
    "    print('Accuracy: %.3f' % acc2)\n",
    "    \n",
    "    return acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2\n",
    "    \n",
    "def Compare_Model_Scores_Best_Threshold(test_x1, test_x2, y_test, predictions1, predictions2, prediction_probs1, prediction_probs2, model1, model2):\n",
    "    '''\n",
    "    This method provides different metrics about the predictions associated with an independent test variable.\n",
    "    These metrics include: PRC-AUC scores, ROC-AUC scores, and the classification report provided by sklearn\n",
    "    '''\n",
    "\n",
    "\n",
    "    \n",
    "    # ovr: One-vs-rest\n",
    "    # ovo: One-vs-one\n",
    "    print(\"\\nScores for model 1\")\n",
    "    print(\"------------------\")\n",
    "    precision1, recall1, thresholds1 = precision_recall_curve(y_test, prediction_probs1[:,1])\n",
    "    fscore1 = (2 * (np.array(precision1, dtype=float) * np.array(recall1, dtype=float)) / (np.array(precision1, dtype=float) + np.array(recall1, dtype=float)))\n",
    "    fscore1[np.isnan(fscore1)] = 0 \n",
    "    # locate the index of the largest f score\n",
    "    ix1 = np.argmax(fscore1)\n",
    "    \n",
    "    pr_auc1 = auc(recall1[ix1:], precision1[ix1:])\n",
    "    roc_val1 = roc_auc_score(y_test, prediction_probs1[:, 1], multi_class='ovr')\n",
    "    print('Roc_Auc Score: {}'.format(roc_val1))\n",
    "    prc_val1 = average_precision_score(y_test, prediction_probs1[:, 1])\n",
    "    print(\"Average Precision-Recall Score: {}\".format(prc_val1))\n",
    "    print(f\"PRC-AUC for model 1: {pr_auc1}\")\n",
    "    \n",
    "    # Measure the accuracy of the results by comparing the test data with the predictions using the best threshold\n",
    "    acc1 = accuracy_score(y_test, predictions1)\n",
    "    print('Accuracy: %.3f' % acc1)\n",
    "\n",
    "    '''\n",
    "    Classification Report breakdown from https://datascience.stackexchange.com/questions/64441/how-to-interpret-classification-report-of-scikit-learn:\n",
    "    The recall means \"how many of this class you find over the whole number of element of this class\"\n",
    "\n",
    "    The precision will be \"how many are correctly classified among that class\"\n",
    "\n",
    "    The f1-score is the harmonic mean between precision & recall\n",
    "\n",
    "    The support is the number of occurence of the given class in your dataset (so you have 37.5K of class 0 and 37.5K of class 1, which is a really well balanced dataset.\n",
    "    '''\n",
    "\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_test, predictions1))\n",
    "\n",
    "    print(\"\\nScores for model 2\")\n",
    "    print(\"------------------\")\n",
    "    # Temporarily removed to retrieve precision & recall by hand\n",
    "    precision2, recall2, thresholds2 = precision_recall_curve(y_test, prediction_probs2[:, 1])\n",
    "    fscore2 = (2 * (np.array(precision2, dtype=float) * np.array(recall2, dtype=float)) / (np.array(precision2, dtype=float) + np.array(recall2, dtype=float)))\n",
    "    fscore2[np.isnan(fscore2)] = 0  \n",
    "    ix2 = np.argmax(fscore2)\n",
    "    \n",
    "    pr_auc2 = auc(recall2[ix2:], precision2[ix2:])\n",
    "    roc_val2 = roc_auc_score(y_test, prediction_probs2[:, 1], multi_class='ovr')\n",
    "    print('Roc_Auc Score: {}'.format(roc_val2))\n",
    "    prc_val2 = average_precision_score(y_test, prediction_probs2[:, 1])\n",
    "    print(\"Average Precision-Recall Score: {}\".format(prc_val2))\n",
    "    print(f\"PRC-AUC for model 2: {pr_auc2}\")\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_test, predictions2))\n",
    "    \n",
    "    # Measure the accuracy of the results by comparing the test data with the predictions using the best threshold\n",
    "    acc2 = accuracy_score(y_test, predictions2)\n",
    "    print('Accuracy: %.3f' % acc2)\n",
    "    \n",
    "    return acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2\n",
    "\n",
    "def plot_thresholds(model1, model2, test_x1, test_y1, test_x2, test_y2, prediction_probs1, prediction_probs2, title):\n",
    "    '''Predict test_y values and probabilities based on fitted logistic for both models''' \n",
    "\n",
    "    # recall1, recall2, precision1, precision2, threshold_list = get_precision_recall(test_x1, test_x2, test_y1, model1, model2)\n",
    "    \n",
    "    precision1, recall1, thresholds1 = precision_recall_curve(test_y1, prediction_probs1[:, 1]) \n",
    "    precision2, recall2, thresholds2 = precision_recall_curve(test_y2, prediction_probs2[:, 1])\n",
    "    \n",
    "    # convert to f1 score\n",
    "    # from: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "    fscore1 = (2 * (np.array(precision1, dtype=float) * np.array(recall1, dtype=float)) / (np.array(precision1, dtype=float) + np.array(recall1, dtype=float)))\n",
    "    fscore2 = (2 * (np.array(precision2, dtype=float) * np.array(recall2, dtype=float)) / (np.array(precision2, dtype=float) + np.array(recall2, dtype=float)))\n",
    "    fscore1[np.isnan(fscore1)] = 0 \n",
    "    fscore2[np.isnan(fscore2)] = 0 \n",
    "    \n",
    "    # locate the index of the largest f score\n",
    "    ix1 = np.argmax(fscore1)\n",
    "    ix2 = np.argmax(fscore2)\n",
    "    #print(f\"F score 1: {fscore1} with ix: {ix1}\")\n",
    "    #print(f\"F score 2: {fscore2} with ix: {ix2}\")\n",
    "    print('Best Threshold=%f, F1-Score=%.3f for model 1' % (thresholds1[ix1], fscore1[ix1]))\n",
    "    print('Best Threshold=%f, F1-Score=%.3f for model 2' % (thresholds2[ix2], fscore2[ix2]))\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.title(f\"PRC for {title}\")\n",
    "    # use marker = \".\" to see each threshhold value\n",
    "    # Shows the PRC-AUC from the best threshold onward, rather than the entire curve\n",
    "    plt.plot(recall1[:-1], precision1[:-1], \"b\", label=f\"Model 1\\n-----------\\n • PRC-AUC score: {auc(recall1, precision1):.2f}\\n • Best Threshold: {round(thresholds1[ix1], 2):.2f}\\n • PRC-AUC to threshold: {auc(recall1[ix1:], precision1[ix1:]):.2f}\\n • Best F1-Score: {round(fscore1[ix1], 2):.2f}\\n\")\n",
    "    plt.plot(recall2[:-1], precision2[:-1], \"r--\", label=f\"Model 2\\n-----------\\n • PRC-AUC score: {auc(recall2, precision2):.2f}\\n • Best Threshold: {round(thresholds2[ix2], 2):.2f}\\n • PRC-AUC to threshold: {auc(recall2[ix2:], precision2[ix2:]):.2f}\\n • Best F1-Score: {round(fscore2[ix2], 2):.2f}\")\n",
    "    plt.scatter([recall1[ix1], recall2[ix2]], [precision1[ix1], precision2[ix2]], marker='o', color='black', label='Best threshold')\n",
    "    #plt.annotate('Model 1 Best Threshold=%.2f, Best F1-Score=%.2f' % (thresholds1[ix1], fscore1[ix1]), (0.38, 0.35), fontsize=8)\n",
    "    #plt.annotate('Model 2 Best Threshold=%.2f, Best F1-Score=%.2f' % (thresholds2[ix2], fscore2[ix2]), (0.38, 0.3), fontsize=8)\n",
    "    \n",
    "    x1 = np.array(recall1[ix1:], dtype=float)\n",
    "    x2 = np.array(recall2[ix2:], dtype=float)\n",
    "    y1 = np.array(precision1[ix1:], dtype=float)\n",
    "    y2 = np.array(precision2[ix2:], dtype=float)\n",
    "    y1_opp = np.array(precision1[ix2:], dtype=float)\n",
    "    \n",
    "    #plt.fill_between(x1, y1, color='b', alpha=0.5)\n",
    "    # where=y1_opp<=y2\n",
    "    #plt.fill_between(x2, y2, color='r', alpha=0.3)\n",
    "    \n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([0,1])\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return thresholds1[ix1], thresholds2[ix2], fig\n",
    "    \n",
    "def simple_threshold_plot(classifier, x_test, y_test):\n",
    "    predictions = classifier.predict(x_test)\n",
    "    prc_val = average_precision_score(y_test, predictions)\n",
    "    disp = plot_precision_recall_curve(classifier, x_test, y_test)\n",
    "    disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(prc_val))\n",
    "    \n",
    "def get_precision_recall(test_x1, test_x2, test_y, model1, model2):\n",
    "    '''\n",
    "    Get the the preicison and recall values for every data point with each type of threshold\n",
    "    '''\n",
    "    \n",
    "    recall1, recall2, precision1, precision2 = list(), list(), list(), list()\n",
    "    \n",
    "    # Could also create thresholds using: thresholds = arange(0, 1, 0.001)\n",
    "    # threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "    threshold_list = np.arange(0.001, 1, 0.001)\n",
    "    for threshold in threshold_list:\n",
    "        #pred_y1=model1.predict(test_x1) \n",
    "        probs_y1=(model1.predict_proba(test_x1)[:,1] >= threshold).astype(int)\n",
    "        #pred_y2=model2.predict(test_x2) \n",
    "        probs_y2=(model2.predict_proba(test_x2)[:,1] >= threshold).astype(int)\n",
    "        # probs_y is a 2-D array of probability of being labeled as 0 (first column of array) \n",
    "        # vs 1 (2nd column in array)\n",
    "        precision1.append(precision_score(test_y, probs_y1, average='binary'))\n",
    "        recall1.append(recall_score(test_y, probs_y1, average='binary'))\n",
    "        precision2.append(precision_score(test_y, probs_y2, average='binary'))\n",
    "        recall2.append(recall_score(test_y, probs_y2, average='binary'))\n",
    "        \n",
    "    return recall1, recall2, precision1, precision2, threshold_list\n",
    "\n",
    "def get_precision_recall_best_thresh(test_x1, test_x2, test_y, model1, model2, best_thresh1=None, best_thresh2=None):\n",
    "    '''\n",
    "    Get the the preicison and recall values for every data point with the best threshold\n",
    "    '''\n",
    "    limit1, limit2 = 1, 1\n",
    "    if best_thresh1 != None:\n",
    "        limit1 = best_thresh1\n",
    "    if best_thresh2 != None:\n",
    "        limit2 = best_thresh2\n",
    "    \n",
    "    recall1, recall2, precision1, precision2 = list(), list(), list(), list()\n",
    "    \n",
    "    # Could also create thresholds using: thresholds = arange(0, 1, 0.001)\n",
    "    # threshold_list = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,.7,.75,.8,.85,.9,.95,.99]\n",
    "    threshold_list = np.arange(0.001, limit1, 0.001)\n",
    "    for threshold in threshold_list:\n",
    "        #pred_y1=model1.predict(test_x1) \n",
    "        probs_y1=(model1.predict_proba(test_x1)[:,1] >= threshold).astype(int)\n",
    "        # probs_y is a 2-D array of probability of being labeled as 0 (first column of array) \n",
    "        # vs 1 (2nd column in array)\n",
    "        precision1.append(precision_score(test_y, probs_y1, average='binary'))\n",
    "        recall1.append(recall_score(test_y, probs_y1, average='binary'))\n",
    "        \n",
    "    threshold_list = np.arange(0.001, limit2, 0.001)\n",
    "    for threshold in threshold_list:\n",
    "        #pred_y2=model2.predict(test_x2) \n",
    "        probs_y2=(model2.predict_proba(test_x2)[:,1] >= threshold).astype(int)\n",
    "        # probs_y is a 2-D array of probability of being labeled as 0 (first column of array) \n",
    "        # vs 1 (2nd column in array)\n",
    "        precision2.append(precision_score(test_y, probs_y2, average='binary'))\n",
    "        recall2.append(recall_score(test_y, probs_y2, average='binary'))\n",
    "    return recall1, recall2, precision1, precision2, threshold_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph-Based Analysis using Logistic Regression, Random Forest Classifer, and XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of all of the unique commits (i.e. links) between developers and the corresponding folders\n",
    "use_cols = [\"Bug\", \"PageRank\", \"Betweenness\", \"Closeness\", \"Harmonic\", \"Degree\", \"communityId\", \"n2vEmbedding\"]\n",
    "ignore_cols = [\"Name\", \"File\"]\n",
    "\n",
    "graph_df = pd.read_csv(\"../Neo4j_output/JiT_Reliability_Output/combined_output.csv\", usecols=use_cols)\n",
    "\n",
    "new_columns = {}\n",
    "\n",
    "embedding_list = [f\"emb_{i}\" for i in range(128)]\n",
    "\n",
    "# Generate binary classification for our dataframe based on if a developer \n",
    "# introduced a bug or not with the corresponding commit\n",
    "for index in graph_df.index:\n",
    "    if graph_df.loc[index, \"Bug\"] != \"INTRODUCED_NEW_BUG\":\n",
    "        graph_df.loc[index, \"Bug\"] = 0\n",
    "    else:\n",
    "        graph_df.loc[index, \"Bug\"] = 1\n",
    "\n",
    "\n",
    "    '''\n",
    "    Brute force method for extrapolating embeddings\n",
    "    # separate each node2vec embedding into it's own unique label\n",
    "    embeddings = literal_eval(graph_df.loc[index, 'n2vEmbedding'])\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if f\"emb_{i}\" not in new_columns:\n",
    "            new_columns[f\"emb_{i}\"] = []\n",
    "            new_columns[f\"emb_{i}\"].append(embedding)\n",
    "        else:\n",
    "            new_columns[f\"emb_{i}\"].append(embedding)\n",
    "    '''\n",
    "'''\n",
    "Part of the old method\n",
    "# delete the n2vEmbedding label, as the list has now been separated into their own unique labels \n",
    "del graph_df['n2vEmbedding']\n",
    "temp_df = pd.DataFrame.from_dict(new_columns)\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame(graph_df, columns=\"n2vEmbedding\")\n",
    "temp_df[embedding_list] = pd.DataFrame(temp_df.n2vEmbedding.tolist(), index=temp_df.index)\n",
    "\n",
    "temp_df2 = pd.DataFrame(temp_df['n2vEmbedding'].to_list(), columns=embedding_list)\n",
    "\n",
    "graph_df = graph_df.join(temp_df2)\n",
    "            \n",
    "#x = graph_df[\"Name\"]\n",
    "y = graph_df[\"Bug\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph_df['Bug'].value_counts())\n",
    "# 2D Array containing all results\n",
    "results_data = [[None for j in range(9)] for i in range(72)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1 = LogisticRegression(solver='liblinear', random_state=0)\n",
    "lr_model2 = LogisticRegression(solver='liblinear', random_state=0)\n",
    "rf_model1 = RandomForestClassifier(n_estimators=120)\n",
    "rf_model2 = RandomForestClassifier(n_estimators=120)\n",
    "xgb_model1 = XGBClassifier(verbosity = 0)\n",
    "xgb_model2 = XGBClassifier(verbosity = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_test_split params\n",
    "-----------------------\n",
    "graph_df: Graph dataset\n",
    "test_size: float value between 0.0 and 1.0 representing the precentage of data to be put into the test dataset\n",
    "random_state = used to create reproducible, or deterministic results.\n",
    "'''\n",
    "train, test = train_test_split(graph_df, test_size=0.3, random_state = 5)\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "# Labels used for model 1\n",
    "labels1 = ['PageRank', 'Betweenness', 'Closeness', 'Harmonic', 'Degree']\n",
    "\n",
    "# Labels used for model 2\n",
    "labels2 = set(list(graph_df.columns))\n",
    "labels2.difference_update(['index', 'Bug', 'PageRank', 'Betweenness', 'Closeness', 'Harmonic', 'Degree'])\n",
    "\n",
    "x1_train = train[labels1]\n",
    "x2_train = train[labels2]\n",
    "y_train = train[\"Bug\"]\n",
    "y_train = y_train.astype('int')\n",
    "x1_test = test[labels1]\n",
    "x2_test = test[labels2]\n",
    "y_test = test[\"Bug\"]\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['Bug'].value_counts())\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the class counts for our binary classification in the training dataset. In this case, our results are 12,060 <b>False (0)</b> counts, and 567 <b>True (1)</b> counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test['Bug'].value_counts())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1.fit(x1_train, y_train)\n",
    "lr_model2.fit(x2_train, y_train)\n",
    "rf_model1.fit(x1_train, y_train)\n",
    "rf_model2.fit(x2_train, y_train)\n",
    "xgb_model1.fit(x1_train, y_train)\n",
    "xgb_model2.fit(x2_train, y_train)\n",
    "\n",
    "lr_predictions1 = lr_model1.predict(x1_test)\n",
    "lr_predictions2 = lr_model2.predict(x2_test)\n",
    "lr_prediction_probs1 = lr_model1.predict_proba(x1_test)\n",
    "lr_prediction_probs2 = lr_model2.predict_proba(x2_test)\n",
    "\n",
    "rf_predictions1 = rf_model1.predict(x1_test)\n",
    "rf_predictions2 = rf_model2.predict(x2_test)\n",
    "rf_prediction_probs1 = rf_model1.predict_proba(x1_test)\n",
    "rf_prediction_probs2 = rf_model2.predict_proba(x2_test)\n",
    "\n",
    "xgb_predictions1 = xgb_model1.predict(x1_test)\n",
    "xgb_predictions2 = xgb_model2.predict(x2_test)\n",
    "xgb_prediction_probs1 = xgb_model1.predict_proba(x1_test)\n",
    "xgb_prediction_probs2 = xgb_model2.predict_proba(x2_test)\n",
    "\n",
    "# Score returns the mean accuracy on the given test data and labels for the provided model.\n",
    "print(f\"Logistic regression training score for model 1: {lr_model1.score(x1_test, y_test)}\")\n",
    "print(f\"Logistic regression training score for model 2: {lr_model2.score(x2_test, y_test)}\")\n",
    "results_data[0][0] = \"Original_Logistic_Regression\"\n",
    "results_data[0][1] = lr_model1.score(x1_test, y_test)\n",
    "results_data[0][2] = lr_model2.score(x2_test, y_test)\n",
    "\n",
    "print(f\"Random Forrest Classification training score for model 1: {rf_model1.score(x1_test, y_test)}\")\n",
    "print(f\"Random Forrest Classification training score for model 2: {rf_model2.score(x2_test, y_test)}\")\n",
    "results_data[1][0] = \"Original_Random_Forrest\"\n",
    "results_data[1][1] = rf_model1.score(x1_test, y_test)\n",
    "results_data[1][2] = rf_model2.score(x2_test, y_test)\n",
    "\n",
    "print(f\"XGB Classifier training score for model 1: {xgb_model1.score(x1_test, y_test)}\")\n",
    "print(f\"XGB Classifier training score for model 2: {xgb_model2.score(x2_test, y_test)}\")\n",
    "results_data[2][0] = \"Original_XGB_Classifier\"\n",
    "results_data[2][1] = xgb_model1.score(x1_test, y_test)\n",
    "results_data[2][2] = xgb_model2.score(x2_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare model scores for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y_test, lr_predictions1, lr_predictions2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "results_data[0][3] = acc1\n",
    "results_data[0][4] = acc2\n",
    "results_data[0][5] = prc_val1\n",
    "results_data[0][6] = prc_val2\n",
    "results_data[0][7] = pr_auc1\n",
    "results_data[0][8] = pr_auc2\n",
    "\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y_test, rf_predictions1, rf_predictions2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[1][3] = acc1\n",
    "results_data[1][4] = acc2\n",
    "results_data[1][5] = prc_val1\n",
    "results_data[1][6] = prc_val2\n",
    "results_data[1][7] = pr_auc1\n",
    "results_data[1][8] = pr_auc2\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y_test, xgb_predictions1, xgb_predictions2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[2][3] = acc1\n",
    "results_data[2][4] = acc2\n",
    "results_data[2][5] = prc_val1\n",
    "results_data[2][6] = prc_val2\n",
    "results_data[2][7] = pr_auc1\n",
    "results_data[2][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compare Precision-Recall thresholds between models\n",
    "\n",
    "TODO: Get it to work with randomforrest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_best_threshold1, lr_best_threshold2, lr_og_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y_test, x2_test, y_test, lr_prediction_probs1, lr_prediction_probs2, \"original dataset Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_threshold1, rf_best_threshold2, rf_og_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y_test, x2_test, y_test, rf_prediction_probs1, rf_prediction_probs2, \"original dataset Random Forrest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_threshold1, xgb_best_threshold2, xgb_og_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y_test, x2_test, y_test, xgb_prediction_probs1, xgb_prediction_probs2, \"original dataset XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model1, x1_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model2, x2_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best thresholds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction_bestthresh1 = (lr_model1.predict_proba(x1_test)[:,1] >= lr_best_threshold1).astype(int)\n",
    "lr_prediction_bestthresh2 = (lr_model2.predict_proba(x2_test)[:,1] >= lr_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y_test, lr_prediction_bestthresh1, lr_prediction_bestthresh2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "\n",
    "results_data[3][0] = \"Original_Logistic_Regression_Best_Threshold\"\n",
    "results_data[3][1] = lr_model1.score(x1_test, y_test)\n",
    "results_data[3][2] = lr_model2.score(x2_test, y_test)\n",
    "results_data[3][3] = acc1\n",
    "results_data[3][4] = acc2\n",
    "results_data[3][5] = prc_val1\n",
    "results_data[3][6] = prc_val2\n",
    "results_data[3][7] = pr_auc1\n",
    "results_data[3][8] = pr_auc2\n",
    "\n",
    "rf_prediction_bestthresh1 = (rf_model1.predict_proba(x1_test)[:,1] >= rf_best_threshold1).astype(int)\n",
    "rf_prediction_bestthresh2 = (rf_model2.predict_proba(x2_test)[:,1] >= rf_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y_test, rf_prediction_bestthresh1, rf_prediction_bestthresh2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[4][0] = \"Original_Random_Forrest_Best_Threshold\"\n",
    "results_data[4][1] = rf_model1.score(x1_test, y_test)\n",
    "results_data[4][2] = rf_model2.score(x2_test, y_test)\n",
    "results_data[4][3] = acc1\n",
    "results_data[4][4] = acc2\n",
    "results_data[4][5] = prc_val1\n",
    "results_data[4][6] = prc_val2\n",
    "results_data[4][7] = pr_auc1\n",
    "results_data[4][8] = pr_auc2\n",
    "\n",
    "xgb_prediction_bestthresh1 = (xgb_model1.predict_proba(x1_test)[:,1] >= xgb_best_threshold1).astype(int)\n",
    "xgb_prediction_bestthresh2 = (xgb_model2.predict_proba(x2_test)[:,1] >= xgb_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y_test, xgb_prediction_bestthresh1, xgb_prediction_bestthresh2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[5][0] = \"Original_XGBoost_Classifier_Best_Threshold\"\n",
    "results_data[5][1] = xgb_model1.score(x1_test, y_test)\n",
    "results_data[5][2] = xgb_model2.score(x2_test, y_test)\n",
    "results_data[5][3] = acc1\n",
    "results_data[5][4] = acc2\n",
    "results_data[5][5] = prc_val1\n",
    "results_data[5][6] = prc_val2\n",
    "results_data[5][7] = pr_auc1\n",
    "results_data[5][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = graph_df[labels1]\n",
    "x2 = graph_df[labels2]\n",
    "y = graph_df['Bug']\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross validation results for model 1\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "cv_results = cross_validate(lr_model1, x1, y, cv=3)\n",
    "sorted(cv_results.keys())\n",
    "print(cv_results['test_score'])\n",
    "\n",
    "print(\"\\nCross validation results for model 2\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "cv_results = cross_validate(lr_model2, x2, y, cv=3)\n",
    "sorted(cv_results.keys())\n",
    "print(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Running extremely slow, working on this issue\n",
    "#Loo(lr_model1, x1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf, pr_auc, lr_rkf_prediction_probs1 = Rkf(lr_model1, x1, y)\n",
    "results_data[6][0] = \"Original_Logistic_Regression_rkf\"\n",
    "results_data[6][1] = model_score\n",
    "results_data[6][3] = acc\n",
    "results_data[6][5] = prc_val\n",
    "results_data[6][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs1 = Rkf(lr_model1, x1, y, lr_best_threshold1)\n",
    "results_data[7][0] = \"Original_Logistic_Regression_rkf_Best_Threshold\"\n",
    "results_data[7][1] = model_score\n",
    "results_data[7][3] = acc\n",
    "results_data[7][5] = prc_val\n",
    "results_data[7][7] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model1, x1, y)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_rkf, pr_auc, rf_rkf_prediction_probs1 = Rkf(rf_model1, x1, y)\n",
    "results_data[8][0] = \"Original_Random_Forrest_rkf\"\n",
    "results_data[8][1] = model_score\n",
    "results_data[8][3] = acc\n",
    "results_data[8][5] = prc_val\n",
    "results_data[8][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs1 = Rkf(xgb_model1, x1, y)\n",
    "results_data[9][0] = \"Original_XGB_Classifier_rkf\"\n",
    "results_data[9][1] = model_score\n",
    "results_data[9][3] = acc\n",
    "results_data[9][5] = prc_val\n",
    "results_data[9][7] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_skf, pr_auc, lr_skf_prediction_probs1 = Skf(lr_model1, x1, y)\n",
    "results_data[10][0] = \"Original_Logistic_Regression_skf\"\n",
    "results_data[10][1] = model_score\n",
    "results_data[10][3] = acc\n",
    "results_data[10][5] = prc_val\n",
    "results_data[10][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf_best, pr_auc, lr_skf_best_prediction_probs1 = Skf(lr_model1, x1, y, lr_best_threshold1)\n",
    "results_data[11][0] = \"Original_Logistic_Regression_skf_Best_Threshold\"\n",
    "results_data[11][1] = model_score\n",
    "results_data[11][3] = acc\n",
    "results_data[11][5] = prc_val\n",
    "results_data[11][7] = pr_auc\n",
    "\n",
    "Skf_short(lr_model1, x1, y)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_skf, pr_auc, rf_skf_prediction_probs1 = Skf(rf_model1, x1, y)\n",
    "results_data[12][0] = \"Original_Random_Forrest_skf\"\n",
    "results_data[12][1] = model_score\n",
    "results_data[12][3] = acc\n",
    "results_data[12][5] = prc_val\n",
    "results_data[12][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_skf, pr_auc, xgb_skf_prediction_probs1 = Skf(xgb_model1, x1, y)\n",
    "results_data[13][0] = \"Original_XGB_Classifier_skf\"\n",
    "results_data[13][1] = model_score\n",
    "results_data[13][3] = acc\n",
    "results_data[13][5] = prc_val\n",
    "results_data[13][7] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss, pr_auc, lr_tss_prediction_probs1 = Tss(lr_model1, x1, y)\n",
    "results_data[14][0] = \"Original_Logistic_Regression_tss\"\n",
    "results_data[14][1] = model_score\n",
    "results_data[14][3] = acc\n",
    "results_data[14][5] = prc_val\n",
    "results_data[14][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs1 = Tss(lr_model1, x1, y, lr_best_threshold1)\n",
    "results_data[15][0] = \"Original_Logistic_Regression_tss_Best_Threshold\"\n",
    "results_data[15][1] = model_score\n",
    "results_data[15][3] = acc\n",
    "results_data[15][5] = prc_val\n",
    "results_data[15][7] = pr_auc\n",
    "\n",
    "Skf_short(lr_model1, x1, y)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_tss, pr_auc, rf_tss_prediction_probs1 = Tss(rf_model1, x1, y)\n",
    "results_data[16][0] = \"Original_Random_Forrest_tss\"\n",
    "results_data[16][1] = model_score\n",
    "results_data[16][3] = acc\n",
    "results_data[16][5] = prc_val\n",
    "results_data[16][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_tss, pr_auc, xgb_tss_prediction_probs1 = Tss(xgb_model1, x1, y)\n",
    "results_data[17][0] = \"Original_XGB_Classifier_tss\"\n",
    "results_data[17][1] = model_score\n",
    "results_data[17][3] = acc\n",
    "results_data[17][5] = prc_val\n",
    "results_data[17][7] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Running extremely slow, working on this issue\n",
    "#Loo(lr_model2, x2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf, pr_auc, lr_rkf_prediction_probs2 = Rkf(lr_model2, x2, y)\n",
    "results_data[6][2] = model_score\n",
    "results_data[6][4] = acc\n",
    "results_data[6][6] = prc_val\n",
    "results_data[6][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs2 = Rkf(lr_model2, x2, y, lr_best_threshold2)\n",
    "results_data[7][2] = model_score\n",
    "results_data[7][4] = acc\n",
    "results_data[7][6] = prc_val\n",
    "results_data[7][8] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model2, x2, y)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_rkf, pr_auc, rf_rkf_prediction_probs2 = Rkf(rf_model2, x2, y)\n",
    "results_data[8][2] = model_score\n",
    "results_data[8][4] = acc\n",
    "results_data[8][6] = prc_val\n",
    "results_data[8][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs2 = Rkf(xgb_model2, x2, y)\n",
    "results_data[9][2] = model_score\n",
    "results_data[9][4] = acc\n",
    "results_data[9][6] = prc_val\n",
    "results_data[9][8] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf, pr_auc, lr_skf_prediction_probs2 = Skf(lr_model2, x2, y)\n",
    "results_data[10][2] = model_score\n",
    "results_data[10][4] = acc\n",
    "results_data[10][6] = prc_val\n",
    "results_data[10][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf_best, pr_auc, lr_skf_best_prediction_probs2 = Skf(lr_model2, x2, y, lr_best_threshold2)\n",
    "results_data[11][2] = model_score\n",
    "results_data[11][4] = acc\n",
    "results_data[11][6] = prc_val\n",
    "results_data[11][8] = pr_auc\n",
    "\n",
    "Skf_short(lr_model2, x2, y)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_skf, pr_auc, rf_skf_prediction_probs2 = Skf(rf_model2, x2, y)\n",
    "results_data[12][2] = model_score\n",
    "results_data[12][4] = acc\n",
    "results_data[12][6] = prc_val\n",
    "results_data[12][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_skf, pr_auc, xgb_skf_prediction_probs2 = Skf(xgb_model2, x2, y)\n",
    "results_data[13][2] = model_score\n",
    "results_data[13][4] = acc\n",
    "results_data[13][6] = prc_val\n",
    "results_data[13][8] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss, pr_auc, lr_tss_prediction_probs2 = Tss(lr_model2, x2, y)\n",
    "results_data[14][2] = model_score\n",
    "results_data[14][4] = acc\n",
    "results_data[14][6] = prc_val\n",
    "results_data[14][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs2 = Tss(lr_model2, x2, y, lr_best_threshold2)\n",
    "results_data[15][2] = model_score\n",
    "results_data[15][4] = acc\n",
    "results_data[15][6] = prc_val\n",
    "results_data[15][8] = pr_auc\n",
    "\n",
    "Skf_short(lr_model2, x2, y)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_tss, pr_auc, rf_tss_prediction_probs2 = Tss(rf_model2, x2, y)\n",
    "results_data[16][2] = model_score\n",
    "results_data[16][4] = acc\n",
    "results_data[16][6] = prc_val\n",
    "results_data[16][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_tss, pr_auc, xgb_tss_prediction_probs2 = Tss(xgb_model2, x2, y)\n",
    "results_data[17][2] = model_score\n",
    "results_data[17][4] = acc\n",
    "results_data[17][6] = prc_val\n",
    "results_data[17][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lr_rkf_best_threshold1, lr_rkf_best_threshold2, lr_rkf_og_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_rkf, x2_test, y2_lr_test_rkf, lr_rkf_prediction_probs1, lr_rkf_prediction_probs2, \"original dataset Logistic Regression Rkf\")\n",
    "#rf_rkf_best_threshold1, rf_rkf_best_threshold2, rf_rkf_og_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_rkf, x2_test, y2_rf_test_rkf, rf_rkf_prediction_probs1, rf_rkf_prediction_probs2, \"original dataset Random Forrest Rkf\")\n",
    "#xgb_rkf_best_threshold1, xgb_rkf_best_threshold2, xgb_rkf_og_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_rkf, x2_test, y2_xgb_test_rkf, xgb_rkf_prediction_probs1, xgb_rkf_prediction_probs2, \"original dataset XGBoost Rkf\")\n",
    "\n",
    "lr_skf_best_threshold1, lr_skf_best_threshold2, lr_skf_og_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_skf, x2_test, y2_lr_test_skf, lr_skf_prediction_probs1, lr_skf_prediction_probs2, \"original dataset Logistic Regression Skf\")\n",
    "rf_skf_best_threshold1, rf_skf_best_threshold2, rf_skf_og_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_skf, x2_test, y2_rf_test_skf, rf_skf_prediction_probs1, rf_skf_prediction_probs2, \"original dataset Random Forrest Skf\")\n",
    "xgb_skf_best_threshold1, xgb_skf_best_threshold2, xgb_skf_og_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_skf, x2_test, y2_xgb_test_skf, xgb_skf_prediction_probs1, xgb_skf_prediction_probs2, \"original dataset XGBoost Skf\")\n",
    "\n",
    "lr_tss_best_threshold1, lr_tss_best_threshold2, lr_tss_og_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_tss, x2_test, y2_lr_test_tss, lr_tss_prediction_probs1, lr_tss_prediction_probs2, \"original dataset Logistic Regression Tss\")\n",
    "rf_tss_best_threshold1, rf_tss_best_threshold2, rf_tss_og_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_tss, x2_test, y2_rf_test_tss, rf_tss_prediction_probs1, rf_tss_prediction_probs2, \"original dataset Random Forrest Tss\")\n",
    "xgb_tss_best_threshold1, xgb_tss_best_threshold2, xgb_tss_og_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_tss, x2_test, y2_xgb_test_tss, xgb_tss_prediction_probs1, xgb_tss_prediction_probs2, \"original dataset XGBoost Tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebalancing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_df = graph_df.loc[graph_df['Bug'] == 1].sample(n=1900, random_state=42)\n",
    "non_bug_df = graph_df.loc[graph_df['Bug'] == 0].sample(n=1900, random_state=42)\n",
    "normalized_under_df = pd.concat([bug_df, non_bug_df])\n",
    "normalized_under_df = normalized_under_df.reset_index()\n",
    "print(normalized_under_df['Bug'].value_counts())\n",
    "\n",
    "usx1 = normalized_under_df[labels1]\n",
    "usx2 = normalized_under_df[labels2]\n",
    "usy = normalized_under_df[\"Bug\"]\n",
    "usy = usy.sample(frac=1).reset_index(drop=True) # shuffle dataset\n",
    "usy = usy.astype('int')\n",
    "\n",
    "train, test = train_test_split(normalized_under_df, test_size=0.3, random_state = 5)\n",
    "\n",
    "labels1 = ['PageRank', 'Betweenness', 'Closeness', 'Harmonic', 'Degree']\n",
    "labels2 = set(list(normalized_under_df.columns))\n",
    "labels2.difference_update(['index', 'Bug', 'PageRank', 'Betweenness', 'Closeness', 'Harmonic', 'Degree'])\n",
    "\n",
    "x1_train = train[labels1]\n",
    "x2_train = train[labels2]\n",
    "y_train = train[\"Bug\"]\n",
    "y_train = y_train.astype('int')\n",
    "x1_test = test[labels1]\n",
    "x2_test = test[labels2]\n",
    "y_test = test[\"Bug\"]\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1.fit(x1_train, y_train)\n",
    "lr_model2.fit(x2_train, y_train)\n",
    "rf_model1.fit(x1_train, y_train)\n",
    "rf_model2.fit(x2_train, y_train)\n",
    "xgb_model1.fit(x1_train, y_train)\n",
    "xgb_model2.fit(x2_train, y_train)\n",
    "\n",
    "lr_predictions1 = lr_model1.predict(x1_test)\n",
    "lr_predictions2 = lr_model2.predict(x2_test)\n",
    "lr_prediction_probs1 = lr_model1.predict_proba(x1_test)\n",
    "lr_prediction_probs2 = lr_model2.predict_proba(x2_test)\n",
    "\n",
    "rf_predictions1 = rf_model1.predict(x1_test)\n",
    "rf_predictions2 = rf_model2.predict(x2_test)\n",
    "rf_prediction_probs1 = rf_model1.predict_proba(x1_test)\n",
    "rf_prediction_probs2 = rf_model2.predict_proba(x2_test)\n",
    "\n",
    "xgb_predictions1 = xgb_model1.predict(x1_test)\n",
    "xgb_predictions2 = xgb_model2.predict(x2_test)\n",
    "xgb_prediction_probs1 = xgb_model1.predict_proba(x1_test)\n",
    "xgb_prediction_probs2 = xgb_model2.predict_proba(x2_test)\n",
    "\n",
    "# Score returns the mean accuracy on the given test data and labels for the provided model.\n",
    "print(f\"Logistic regression training score for model 1: {lr_model1.score(x1_test, y_test)}\")\n",
    "print(f\"Logistic regression training score for model 2: {lr_model2.score(x2_test, y_test)}\")\n",
    "results_data[18][0] = \"Undersampled_Logistic_Regression\"\n",
    "results_data[18][1] = lr_model1.score(x1_test, y_test)\n",
    "results_data[18][2] = lr_model2.score(x2_test, y_test)\n",
    "\n",
    "print(f\"Random Forrest Classification training score for model 1: {rf_model1.score(x1_test, y_test)}\")\n",
    "print(f\"Random Forrest Classification training score for model 2: {rf_model2.score(x2_test, y_test)}\")\n",
    "results_data[19][0] = \"Undersampled_Random_Forrest\"\n",
    "results_data[19][1] = rf_model1.score(x1_test, y_test)\n",
    "results_data[19][2] = rf_model2.score(x2_test, y_test)\n",
    "\n",
    "print(f\"XGB Classifier training score for model 1: {xgb_model1.score(x1_test, y_test)}\")\n",
    "print(f\"XGB Classifier training score for model 2: {xgb_model2.score(x2_test, y_test)}\")\n",
    "results_data[20][0] = \"Undersampled_XGB_Classifier\"\n",
    "results_data[20][1] = xgb_model1.score(x1_test, y_test)\n",
    "results_data[20][2] = xgb_model2.score(x2_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y_test, lr_predictions1, lr_predictions2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "results_data[18][3] = acc1\n",
    "results_data[18][4] = acc2\n",
    "results_data[18][5] = prc_val1\n",
    "results_data[18][6] = prc_val2\n",
    "results_data[18][7] = pr_auc1\n",
    "results_data[18][8] = pr_auc2\n",
    "\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y_test, rf_predictions1, rf_predictions2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[19][3] = acc1\n",
    "results_data[19][4] = acc2\n",
    "results_data[19][5] = prc_val1\n",
    "results_data[19][6] = prc_val2\n",
    "results_data[19][7] = pr_auc1\n",
    "results_data[19][8] = pr_auc2\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y_test, xgb_predictions1, xgb_predictions2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[20][3] = acc1\n",
    "results_data[20][4] = acc2\n",
    "results_data[20][5] = prc_val1\n",
    "results_data[20][6] = prc_val2\n",
    "results_data[20][7] = pr_auc1\n",
    "results_data[20][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compare Precision-Recall thresholds between models for undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_threshold1, lr_best_threshold2, lr_us_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y_test, x2_test, y_test, lr_prediction_probs1, lr_prediction_probs2, \"undersampled dataset Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_threshold1, rf_best_threshold2, rf_us_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y_test, x2_test, y_test, rf_prediction_probs1, rf_prediction_probs2, \"undersampled datase Random Forrest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_threshold1, rf_best_threshold2, xgb_us_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y_test, x2_test, y_test, xgb_prediction_probs1, xgb_prediction_probs2, \"undersampled dataset XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model1, x1_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model2, x2_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction_bestthresh1 = (lr_model1.predict_proba(x1_test)[:,1] >= lr_best_threshold1).astype(int)\n",
    "lr_prediction_bestthresh2 = (lr_model2.predict_proba(x2_test)[:,1] >= lr_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y_test, lr_prediction_bestthresh1, lr_prediction_bestthresh2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "results_data[21][0] = \"Undersampled_Logistic_Regression_Best_Threshold\"\n",
    "results_data[21][1] = lr_model1.score(x1_test, y_test)\n",
    "results_data[21][2] = lr_model2.score(x2_test, y_test)\n",
    "results_data[21][3] = acc1\n",
    "results_data[21][4] = acc2\n",
    "results_data[21][5] = prc_val1\n",
    "results_data[21][6] = prc_val2\n",
    "results_data[21][7] = pr_auc1\n",
    "results_data[21][8] = pr_auc2\n",
    "\n",
    "rf_prediction_bestthresh1 = (rf_model1.predict_proba(x1_test)[:,1] >= rf_best_threshold1).astype(int)\n",
    "rf_prediction_bestthresh2 = (rf_model2.predict_proba(x2_test)[:,1] >= rf_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y_test, rf_prediction_bestthresh1, rf_prediction_bestthresh2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[22][0] = \"Undersampled_Random_Forrest_Best_Threshold\"\n",
    "results_data[22][1] = rf_model1.score(x1_test, y_test)\n",
    "results_data[22][2] = rf_model2.score(x2_test, y_test)\n",
    "results_data[22][3] = acc1\n",
    "results_data[22][4] = acc2\n",
    "results_data[22][5] = prc_val1\n",
    "results_data[22][6] = prc_val2\n",
    "results_data[22][7] = pr_auc1\n",
    "results_data[22][8] = pr_auc2\n",
    "\n",
    "xgb_prediction_bestthresh1 = (xgb_model1.predict_proba(x1_test)[:,1] >= xgb_best_threshold1).astype(int)\n",
    "xgb_prediction_bestthresh2 = (xgb_model2.predict_proba(x2_test)[:,1] >= xgb_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y_test, xgb_prediction_bestthresh1, xgb_prediction_bestthresh2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[23][0] = \"Undersampled_XGB_Classifier_Best_Threshold\"\n",
    "results_data[23][1] = xgb_model1.score(x1_test, y_test)\n",
    "results_data[23][2] = xgb_model2.score(x2_test, y_test)\n",
    "results_data[23][3] = acc1\n",
    "results_data[23][4] = acc2\n",
    "results_data[23][5] = prc_val1\n",
    "results_data[23][6] = prc_val2\n",
    "results_data[23][7] = pr_auc1\n",
    "results_data[23][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation After Undersampling Rebalance for model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Repeated KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf, pr_auc, lr_rkf_prediction_probs1 = Rkf(lr_model1, usx1, usy)\n",
    "results_data[24][0] = \"Undersampled_Logistic_Regression_rkf\"\n",
    "results_data[24][1] = model_score\n",
    "results_data[24][3] = acc\n",
    "results_data[24][5] = prc_val\n",
    "results_data[24][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs1 = Rkf(lr_model1, usx1, usy, lr_best_threshold1)\n",
    "results_data[25][0] = \"Undersampled_Logistic_Regression_rkf_Best_Threshold\"\n",
    "results_data[25][1] = model_score\n",
    "results_data[25][3] = acc\n",
    "results_data[25][5] = prc_val\n",
    "results_data[25][7] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model1, usx1, usy)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_rkf, pr_auc, rf_rkf_prediction_probs1 = Rkf(rf_model1, usx1, usy)\n",
    "results_data[26][0] = \"Undersampled_Random_Forrest_rkf\"\n",
    "results_data[26][1] = model_score\n",
    "results_data[26][3] = acc\n",
    "results_data[26][5] = prc_val\n",
    "results_data[26][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs1 = Rkf(xgb_model1, usx1, usy)\n",
    "results_data[27][0] = \"Undersampled_XGB_Classifier_rkf\"\n",
    "results_data[27][1] = model_score\n",
    "results_data[27][3] = acc\n",
    "results_data[27][5] = prc_val\n",
    "results_data[27][7] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_skf, pr_auc, lr_skf_prediction_probs1 = Skf(lr_model1, usx1, usy)\n",
    "results_data[28][0] = \"Undersampled_Logistic_Regression_skf\"\n",
    "results_data[28][1] = model_score\n",
    "results_data[28][3] = acc\n",
    "results_data[28][5] = prc_val\n",
    "results_data[28][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_skf_best, pr_auc, lr_skf_best_prediction_probs1 = Skf(lr_model1, usx1, usy, lr_best_threshold1)\n",
    "results_data[29][0] = \"Undersampled_Logistic_Regression_skf_Best_Threshold\"\n",
    "results_data[29][1] = model_score\n",
    "results_data[29][3] = acc\n",
    "results_data[29][5] = prc_val\n",
    "results_data[29][7] = pr_auc\n",
    "\n",
    "Skf_short(lr_model1, usx1, usy)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_skf, pr_auc, rf_skf_prediction_probs1 = Skf(rf_model1, usx1, usy)\n",
    "results_data[30][0] = \"Undersampled_Random_Forrest_skf\"\n",
    "results_data[30][1] = model_score\n",
    "results_data[30][3] = acc\n",
    "results_data[30][5] = prc_val\n",
    "results_data[30][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_skf, pr_auc, xgb_skf_prediction_probs1 = Skf(xgb_model1, usx1, usy)\n",
    "results_data[31][0] = \"Undersampled_XGB_Classifier_skf\"\n",
    "results_data[31][1] = model_score\n",
    "results_data[31][3] = acc\n",
    "results_data[31][5] = prc_val\n",
    "results_data[31][7] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss, pr_auc, lr_tss_prediction_probs1 = Tss(lr_model1, usx1, usy)\n",
    "results_data[32][0] = \"Undersampled_Logistic_Regression_tss\"\n",
    "results_data[32][1] = model_score\n",
    "results_data[32][3] = acc\n",
    "results_data[32][5] = prc_val\n",
    "results_data[32][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs1 = Tss(lr_model1, usx1, usy, lr_best_threshold1)\n",
    "results_data[33][0] = \"Undersampled_Logistic_Regression_tss_Best_Threshold\"\n",
    "results_data[33][1] = model_score\n",
    "results_data[33][3] = acc\n",
    "results_data[33][5] = prc_val\n",
    "results_data[33][7] = pr_auc\n",
    "\n",
    "Skf_short(lr_model1, usx1, usy)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_tss, pr_auc, rf_tss_prediction_probs1 = Tss(rf_model1, usx1, usy)\n",
    "results_data[34][0] = \"Undersampled_Random_Forrest_tss\"\n",
    "results_data[34][1] = model_score\n",
    "results_data[34][3] = acc\n",
    "results_data[34][5] = prc_val\n",
    "results_data[34][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_tss, pr_auc, xgb_tss_prediction_probs1 = Tss(xgb_model1, usx1, usy)\n",
    "results_data[35][0] = \"Undersampled_XGB_Classifier_tss\"\n",
    "results_data[35][1] = model_score\n",
    "results_data[35][3] = acc\n",
    "results_data[35][5] = prc_val\n",
    "results_data[35][7] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation After Undersampling Rebalance for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf, pr_auc, lr_rkf_prediction_probs2 = Rkf(lr_model2, usx2, usy)\n",
    "results_data[24][2] = model_score\n",
    "results_data[24][4] = acc\n",
    "results_data[24][6] = prc_val\n",
    "results_data[24][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs2 = Rkf(lr_model2, usx2, usy, lr_best_threshold2)\n",
    "results_data[25][2] = model_score\n",
    "results_data[25][4] = acc\n",
    "results_data[25][6] = prc_val\n",
    "results_data[25][8] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model2, usx2, usy)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_rkf, pr_auc, rf_rkf_prediction_probs2 = Rkf(rf_model2, usx2, usy)\n",
    "results_data[26][2] = model_score\n",
    "results_data[26][4] = acc\n",
    "results_data[26][6] = prc_val\n",
    "results_data[26][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs2 = Rkf(xgb_model2, usx2, usy)\n",
    "results_data[27][2] = model_score\n",
    "results_data[27][4] = acc\n",
    "results_data[27][6] = prc_val\n",
    "results_data[27][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf, pr_auc, lr_skf_prediction_probs2 = Skf(lr_model2, usx2, usy)\n",
    "results_data[28][2] = model_score\n",
    "results_data[28][4] = acc\n",
    "results_data[28][6] = prc_val\n",
    "results_data[28][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf_best, pr_auc, lr_skf_best_prediction_probs2 = Skf(lr_model2, usx2, usy, lr_best_threshold2)\n",
    "results_data[29][2] = model_score\n",
    "results_data[29][4] = acc\n",
    "results_data[29][6] = prc_val\n",
    "results_data[29][8] = pr_auc\n",
    "\n",
    "Skf_short(lr_model2, usx2, usy)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_skf, pr_auc, rf_skf_prediction_probs2 = Skf(rf_model2, usx2, usy)\n",
    "results_data[30][2] = model_score\n",
    "results_data[30][4] = acc\n",
    "results_data[30][6] = prc_val\n",
    "results_data[30][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_skf, pr_auc, xgb_skf_prediction_probs2 = Skf(xgb_model2, usx2, usy)\n",
    "results_data[31][2] = model_score\n",
    "results_data[31][4] = acc\n",
    "results_data[31][6] = prc_val\n",
    "results_data[31][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss, pr_auc, lr_tss_prediction_probs2 = Tss(lr_model2, usx2, usy)\n",
    "results_data[32][2] = model_score\n",
    "results_data[32][4] = acc\n",
    "results_data[32][6] = prc_val\n",
    "results_data[32][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs2 = Tss(lr_model2, usx2, usy, lr_best_threshold2)\n",
    "results_data[33][2] = model_score\n",
    "results_data[33][4] = acc\n",
    "results_data[33][6] = prc_val\n",
    "results_data[33][8] = pr_auc\n",
    "\n",
    "Skf_short(lr_model2, usx2, usy)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_tss, pr_auc, rf_tss_prediction_probs2 = Tss(rf_model2, usx2, usy)\n",
    "results_data[34][2] = model_score\n",
    "results_data[34][4] = acc\n",
    "results_data[34][6] = prc_val\n",
    "results_data[34][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_tss, pr_auc, xgb_tss_prediction_probs2 = Tss(xgb_model2, usx2, usy)\n",
    "results_data[35][2] = model_score\n",
    "results_data[35][4] = acc\n",
    "results_data[35][6] = prc_val\n",
    "results_data[35][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_rkf_best_threshold1, lr_rkf_best_threshold2, lr_rkf_us_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_rkf, x2_test, y2_lr_test_rkf, lr_rkf_prediction_probs1, lr_rkf_prediction_probs2, \"Undersampled Logistic Regression Rkf\")\n",
    "#rf_rkf_best_threshold1, rf_rkf_best_threshold2, rf_rkf_us_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_rkf, x2_test, y2_rf_test_rkf, rf_rkf_prediction_probs1, rf_rkf_prediction_probs2, \"Undersampled Random Forrest Rkf\")\n",
    "#xgb_rkf_best_threshold1, xgb_rkf_best_threshold2, xgb_rkf_us_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_rkf, x2_test, y2_xgb_test_rkf, xgb_rkf_prediction_probs1, xgb_rkf_prediction_probs2, \"Undersampled XGBoost Rkf\")\n",
    "\n",
    "lr_skf_best_threshold1, lr_skf_best_threshold2, lr_skf_us_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_skf, x2_test, y2_lr_test_skf, lr_skf_prediction_probs1, lr_skf_prediction_probs2, \"Undersampled Logistic Regression Skf\")\n",
    "rf_skf_best_threshold1, rf_skf_best_threshold2, rf_skf_us_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_skf, x2_test, y2_rf_test_skf, rf_skf_prediction_probs1, rf_skf_prediction_probs2, \"Undersampled Random Forrest Skf\")\n",
    "xgb_skf_best_threshold1, xgb_skf_best_threshold2, xgb_skf_us_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_skf, x2_test, y2_xgb_test_skf, xgb_skf_prediction_probs1, xgb_skf_prediction_probs2, \"Undersampled XGBoost Skf\")\n",
    "\n",
    "lr_tss_best_threshold1, lr_tss_best_threshold2, lr_tss_us_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_tss, x2_test, y2_lr_test_tss, lr_tss_prediction_probs1, lr_tss_prediction_probs2, \"Undersampled Logistic Regression Tss\")\n",
    "rf_tss_best_threshold1, rf_tss_best_threshold2, rf_tss_us_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_tss, x2_test, y2_rf_test_tss, rf_tss_prediction_probs1, rf_tss_prediction_probs2, \"Undersampled Random Forrest Tss\")\n",
    "xgb_tss_best_threshold1, xgb_tss_best_threshold2, xgb_tss_us_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_tss, x2_test, y2_xgb_test_tss, xgb_tss_prediction_probs1, xgb_tss_prediction_probs2, \"Undersampled XGBoost Tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = graph_df[labels1]\n",
    "x2 = graph_df[labels2]\n",
    "y = graph_df[\"Bug\"]\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For oversampling we will use SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "# Resample the minority class. You can change the strategy to 'auto' if you are not sure.\n",
    "sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)\n",
    "\n",
    "print(\"x1 Before SMOTE:\")\n",
    "\n",
    "print(x1_train.shape)\n",
    "\n",
    "print(\"x2 Before SMOTE:\")\n",
    "\n",
    "print(x2_train.shape)\n",
    "\n",
    "# Fit the model to generate the data for Model 1.\n",
    "oversampled_X1, oversampled_Y1 = sm.fit_resample(x1, y)\n",
    "\n",
    "# Fit the model to generate the data for Model 2.\n",
    "oversampled_X2, oversampled_Y2 = sm.fit_resample(x2, y)\n",
    "\n",
    "print(\"x1 After SMOTE:\")\n",
    "\n",
    "print(oversampled_X1.shape)\n",
    "\n",
    "print(\"x2 After SMOTE:\")\n",
    "\n",
    "print(oversampled_X2.shape)\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "print(oversampled_Y1.value_counts(normalize=True) * 100)\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "print(oversampled_Y2.value_counts(normalize=True) * 100)\n",
    "\n",
    "osx1 = oversampled_X1\n",
    "osx2 = oversampled_X2\n",
    "osy1 = oversampled_Y1\n",
    "osy1 = osy1.astype('int')\n",
    "osy2 = oversampled_Y2\n",
    "osy2 = osy2.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_test, y1_train, y1_test = train_test_split(osx1, osy1, test_size=0.3, random_state = 5)\n",
    "y1_train = y1_train.astype('int')\n",
    "y1_test = y1_test.astype('int')\n",
    "#x1_train, y1_train = sm.fit_resample(x1_train, y1_train)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(osx2, osy2, test_size=0.3, random_state = 5)\n",
    "y2_train = y2_train.astype('int')\n",
    "y2_test = y2_test.astype('int')\n",
    "#x2_train, y2_train = sm.fit_resample(x2_train, y2_train)\n",
    "lr_model1.fit(x1_train, y1_train)\n",
    "lr_model2.fit(x2_train, y2_train)\n",
    "rf_model1.fit(x1_train, y1_train)\n",
    "rf_model2.fit(x2_train, y2_train)\n",
    "xgb_model1.fit(x1_train, y1_train)\n",
    "xgb_model2.fit(x2_train, y2_train)\n",
    "\n",
    "lr_predictions1 = lr_model1.predict(x1_test)\n",
    "lr_predictions2 = lr_model2.predict(x2_test)\n",
    "lr_prediction_probs1 = lr_model1.predict_proba(x1_test)\n",
    "lr_prediction_probs2 = lr_model2.predict_proba(x2_test)\n",
    "\n",
    "rf_predictions1 = rf_model1.predict(x1_test)\n",
    "rf_predictions2 = rf_model2.predict(x2_test)\n",
    "rf_prediction_probs1 = rf_model1.predict_proba(x1_test)\n",
    "rf_prediction_probs2 = rf_model2.predict_proba(x2_test)\n",
    "\n",
    "xgb_predictions1 = xgb_model1.predict(x1_test)\n",
    "xgb_predictions2 = xgb_model2.predict(x2_test)\n",
    "xgb_prediction_probs1 = xgb_model1.predict_proba(x1_test)\n",
    "xgb_prediction_probs2 = xgb_model2.predict_proba(x2_test)\n",
    "\n",
    "# Score returns the mean accuracy on the given test data and labels for the provided model.\n",
    "print(f\"Logistic regression training score for model 1: {lr_model1.score(x1_test, y1_test)}\")\n",
    "print(f\"Logistic regression training score for model 2: {lr_model2.score(x2_test, y2_test)}\")\n",
    "results_data[36][0] = \"Oversampled_Logistic_Regression\"\n",
    "results_data[36][1] = lr_model1.score(x1_test, y1_test)\n",
    "results_data[36][2] = lr_model2.score(x2_test, y2_test)\n",
    "\n",
    "print(f\"Random Forrest Classification training score for model 1: {rf_model1.score(x1_test, y1_test)}\")\n",
    "print(f\"Random Forrest Classification training score for model 2: {rf_model2.score(x2_test, y2_test)}\")\n",
    "results_data[37][0] = \"Oversampled_Random_Forrest\"\n",
    "results_data[37][1] = rf_model1.score(x1_test, y1_test)\n",
    "results_data[37][2] = rf_model2.score(x2_test, y2_test)\n",
    "\n",
    "print(f\"XGB Classifier training score for model 1: {xgb_model1.score(x1_test, y1_test)}\")\n",
    "print(f\"XGB Classifier training score for model 2: {xgb_model2.score(x2_test, y2_test)}\")\n",
    "results_data[38][0] = \"Oversampled_XGB_Classifier\"\n",
    "results_data[38][1] = xgb_model1.score(x1_test, y1_test)\n",
    "results_data[38][2] = xgb_model2.score(x2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y1_test, lr_predictions1, lr_predictions2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "results_data[36][3] = acc1\n",
    "results_data[36][4] = acc2\n",
    "results_data[36][5] = prc_val1\n",
    "results_data[36][6] = prc_val2\n",
    "results_data[36][7] = pr_auc1\n",
    "results_data[36][8] = pr_auc2\n",
    "\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y1_test, rf_predictions1, rf_predictions2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[37][3] = acc1\n",
    "results_data[37][4] = acc2\n",
    "results_data[37][5] = prc_val1\n",
    "results_data[37][6] = prc_val2\n",
    "results_data[37][7] = pr_auc1\n",
    "results_data[37][8] = pr_auc2\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y1_test, xgb_predictions1, xgb_predictions2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[38][3] = acc1\n",
    "results_data[38][4] = acc2\n",
    "results_data[38][5] = prc_val1\n",
    "results_data[38][6] = prc_val2\n",
    "results_data[38][7] = pr_auc1\n",
    "results_data[38][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compare Precision-Recall thresholds between models for oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_threshold1, lr_best_threshold2, lr_os_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_test, x2_test, y2_test, lr_prediction_probs1, lr_prediction_probs2, \"oversampled dataset Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_threshold1, rf_best_threshold2, rf_os_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_test, x2_test, y2_test, rf_prediction_probs1, rf_prediction_probs2, \"oversampled dataset Random Forrest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_threshold1, xgb_best_threshold2, xgb_os_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_test, x2_test, y2_test, xgb_prediction_probs1, xgb_prediction_probs2, \"oversampled dataset XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model1, x1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model2, x2_test, y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using best threshold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction_bestthresh1 = (lr_model1.predict_proba(x1_test)[:,1] >= lr_best_threshold1).astype(int)\n",
    "lr_prediction_bestthresh2 = (lr_model2.predict_proba(x2_test)[:,1] >= lr_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y1_test, lr_prediction_bestthresh1, lr_prediction_bestthresh2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "results_data[39][0] = \"Oversampled_Logistic_Regression_Best_Threshold\"\n",
    "results_data[39][1] = lr_model1.score(x1_test, y1_test)\n",
    "results_data[39][2] = lr_model2.score(x2_test, y2_test)\n",
    "results_data[39][3] = acc1\n",
    "results_data[39][4] = acc2\n",
    "results_data[39][5] = prc_val1\n",
    "results_data[39][6] = prc_val2\n",
    "results_data[39][7] = pr_auc1\n",
    "results_data[39][8] = pr_auc2\n",
    "\n",
    "rf_prediction_bestthresh1 = (rf_model1.predict_proba(x1_test)[:,1] >= rf_best_threshold1).astype(int)\n",
    "rf_prediction_bestthresh2 = (rf_model2.predict_proba(x2_test)[:,1] >= rf_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y1_test, rf_prediction_bestthresh1, rf_prediction_bestthresh2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[40][0] = \"Oversampled_Logistic_Regression_Best_Threshold\"\n",
    "results_data[40][1] = rf_model1.score(x1_test, y1_test)\n",
    "results_data[40][2] = rf_model2.score(x2_test, y2_test)\n",
    "results_data[40][3] = acc1\n",
    "results_data[40][4] = acc2\n",
    "results_data[40][5] = prc_val1\n",
    "results_data[40][6] = prc_val2\n",
    "results_data[40][7] = pr_auc1\n",
    "results_data[40][8] = pr_auc2\n",
    "\n",
    "xgb_prediction_bestthresh1 = (xgb_model1.predict_proba(x1_test)[:,1] >= xgb_best_threshold1).astype(int)\n",
    "xgb_prediction_bestthresh2 = (xgb_model2.predict_proba(x2_test)[:,1] >= xgb_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y1_test, xgb_prediction_bestthresh1, xgb_prediction_bestthresh2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[41][0] = \"Oversampled_Logistic_Regression_Best_Threshold\"\n",
    "results_data[41][1] = xgb_model1.score(x1_test, y1_test)\n",
    "results_data[41][2] = xgb_model2.score(x2_test, y2_test)\n",
    "results_data[41][3] = acc1\n",
    "results_data[41][4] = acc2\n",
    "results_data[41][5] = prc_val1\n",
    "results_data[41][6] = prc_val2\n",
    "results_data[41][7] = pr_auc1\n",
    "results_data[41][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation After Oversampling Rebalance for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf, pr_auc, lr_rkf_prediction_probs1 = Rkf(lr_model1, osx1, osy1)\n",
    "results_data[42][0] = \"Oversampled_Logistic_Regression_rkf\"\n",
    "results_data[42][1] = model_score\n",
    "results_data[42][3] = acc\n",
    "results_data[42][5] = prc_val\n",
    "results_data[42][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs1 = Rkf(lr_model1, osx1, osy1, lr_best_threshold1)\n",
    "results_data[43][0] = \"Oversampled_Logistic_Regression_rkf_Best_Threshold\"\n",
    "results_data[43][1] = model_score\n",
    "results_data[43][3] = acc\n",
    "results_data[43][5] = prc_val\n",
    "results_data[43][7] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model1, osx1, osy1)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_rkf, pr_auc, rf_rkf_prediction_probs1 = Rkf(rf_model1, osx1, osy1)\n",
    "results_data[44][0] = \"Oversampled_Random_Forrest_rkf\"\n",
    "results_data[44][1] = model_score\n",
    "results_data[44][3] = acc\n",
    "results_data[44][5] = prc_val\n",
    "results_data[44][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs1 = Rkf(xgb_model1, osx1, osy1)\n",
    "results_data[45][0] = \"Oversampled_XGB_Classifier_rkf\"\n",
    "results_data[45][1] = model_score\n",
    "results_data[45][3] = acc\n",
    "results_data[45][5] = prc_val\n",
    "results_data[45][7] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_skf, pr_auc, lr_skf_prediction_probs1 = Skf(lr_model1, osx1, osy1)\n",
    "results_data[46][0] = \"Oversampled_Logistic_Regression_skf\"\n",
    "results_data[46][1] = model_score\n",
    "results_data[46][3] = acc\n",
    "results_data[46][5] = prc_val\n",
    "results_data[46][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_skf_best, pr_auc, lr_skf_best_prediction_probs1 = Skf(lr_model1, osx1, osy1, lr_best_threshold1)\n",
    "results_data[47][0] = \"Oversampled_Logistic_Regression_skf_Best_Threshold\"\n",
    "results_data[47][1] = model_score\n",
    "results_data[47][3] = acc\n",
    "results_data[47][5] = prc_val\n",
    "results_data[47][7] = pr_auc\n",
    "\n",
    "Skf_short(lr_model1, osx1, osy1)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_skf, pr_auc, rf_skf_prediction_probs1 = Skf(rf_model1, osx1, osy1)\n",
    "results_data[48][0] = \"Oversampled_Random_Forrest_skf\"\n",
    "results_data[48][1] = model_score\n",
    "results_data[48][3] = acc\n",
    "results_data[48][5] = prc_val\n",
    "results_data[48][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_skf, pr_auc, xgb_skf_prediction_probs1 = Skf(xgb_model1, osx1, osy1)\n",
    "results_data[49][0] = \"Oversampled_XGB_Classifier_skf\"\n",
    "results_data[49][1] = model_score\n",
    "results_data[49][3] = acc\n",
    "results_data[49][5] = prc_val\n",
    "results_data[49][7] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss, pr_auc, lr_tss_prediction_probs1 = Tss(lr_model1, osx1, osy1)\n",
    "results_data[50][0] = \"Oversampled_Logistic_Regression_tss\"\n",
    "results_data[50][1] = model_score\n",
    "results_data[50][3] = acc\n",
    "results_data[50][5] = prc_val\n",
    "results_data[50][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs1 = Tss(lr_model1, osx1, osy1, lr_best_threshold1)\n",
    "results_data[51][0] = \"Oversampled_Logistic_Regression_tss_Best_Threshold\"\n",
    "results_data[51][1] = model_score\n",
    "results_data[51][3] = acc\n",
    "results_data[51][5] = prc_val\n",
    "results_data[51][7] = pr_auc\n",
    "\n",
    "Tss_short(lr_model1, osx1, osy1)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_tss, pr_auc, rf_tss_prediction_probs1 = Tss(rf_model1, osx1, osy1)\n",
    "results_data[52][0] = \"Oversampled_Random_Forrest_tss\"\n",
    "results_data[52][1] = model_score\n",
    "results_data[52][3] = acc\n",
    "results_data[52][5] = prc_val\n",
    "results_data[52][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_tss, pr_auc, xgb_tss_prediction_probs1 = Tss(xgb_model1, osx1, osy1)\n",
    "results_data[53][0] = \"Oversampled_XGB_Classifier_tss\"\n",
    "results_data[53][1] = model_score\n",
    "results_data[53][3] = acc\n",
    "results_data[53][5] = prc_val\n",
    "results_data[53][7] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation After Oversampling Rebalance for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf, pr_auc, lr_rkf_prediction_probs2 = Rkf(lr_model2, osx2, osy2)\n",
    "results_data[42][2] = model_score\n",
    "results_data[42][4] = acc\n",
    "results_data[42][6] = prc_val\n",
    "results_data[42][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs2 = Rkf(lr_model2, osx2, osy2, lr_best_threshold2)\n",
    "results_data[43][2] = model_score\n",
    "results_data[43][4] = acc\n",
    "results_data[43][6] = prc_val\n",
    "results_data[43][8] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model2, osx2, osy2)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_rkf, pr_auc, rf_rkf_prediction_probs2 = Rkf(rf_model2, osx2, osy2)\n",
    "results_data[44][2] = model_score\n",
    "results_data[44][4] = acc\n",
    "results_data[44][6] = prc_val\n",
    "results_data[44][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs2 = Rkf(xgb_model2, osx2, osy2)\n",
    "results_data[45][2] = model_score\n",
    "results_data[45][4] = acc\n",
    "results_data[45][6] = prc_val\n",
    "results_data[45][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf, pr_auc, lr_skf_prediction_probs2 = Skf(lr_model2, osx2, osy2)\n",
    "results_data[46][2] = model_score\n",
    "results_data[46][4] = acc\n",
    "results_data[46][6] = prc_val\n",
    "results_data[46][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf_best, pr_auc, lr_skf_best_prediction_probs2 = Skf(lr_model2, osx2, osy2, lr_best_threshold2)\n",
    "results_data[47][2] = model_score\n",
    "results_data[47][4] = acc\n",
    "results_data[47][6] = prc_val\n",
    "results_data[47][8] = pr_auc\n",
    "\n",
    "Skf_short(lr_model2, osx2, osy2)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_skf, pr_auc, rf_skf_prediction_probs2 = Skf(rf_model2, osx2, osy2)\n",
    "results_data[48][2] = model_score\n",
    "results_data[48][4] = acc\n",
    "results_data[48][6] = prc_val\n",
    "results_data[48][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_skf, pr_auc, xgb_skf_prediction_probs2 = Skf(xgb_model1, osx1, osy1)\n",
    "results_data[49][2] = model_score\n",
    "results_data[49][4] = acc\n",
    "results_data[49][6] = prc_val\n",
    "results_data[49][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss, pr_auc, lr_tss_prediction_probs2 = Tss(lr_model2, osx2, osy2)\n",
    "results_data[50][2] = model_score\n",
    "results_data[50][4] = acc\n",
    "results_data[50][6] = prc_val\n",
    "results_data[50][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs2 = Tss(lr_model2, osx2, osy2, lr_best_threshold2)\n",
    "results_data[51][2] = model_score\n",
    "results_data[51][4] = acc\n",
    "results_data[51][6] = prc_val\n",
    "results_data[51][8] = pr_auc\n",
    "\n",
    "Tss_short(lr_model2, osx2, osy2)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_tss, pr_auc, rf_tss_prediction_probs2 = Tss(rf_model2, osx2, osy2)\n",
    "results_data[52][2] = model_score\n",
    "results_data[52][4] = acc\n",
    "results_data[52][6] = prc_val\n",
    "results_data[52][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_tss, pr_auc, xgb_tss_prediction_probs2 = Tss(xgb_model2, osx2, osy2)\n",
    "results_data[53][2] = model_score\n",
    "results_data[53][4] = acc\n",
    "results_data[53][6] = prc_val\n",
    "results_data[53][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_rkf_best_threshold1, lr_rkf_best_threshold2, lr_rkf_os_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_rkf, x2_test, y2_lr_test_rkf, lr_rkf_prediction_probs1, lr_rkf_prediction_probs2, \"Oversampled Logistic Regression Rkf\")\n",
    "#rf_rkf_best_threshold1, rf_rkf_best_threshold2, rf_rkf_os_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_rkf, x2_test, y2_rf_test_rkf, rf_rkf_prediction_probs1, rf_rkf_prediction_probs2, \"Oversampled Random Forrest Rkf\")\n",
    "#xgb_rkf_best_threshold1, xgb_rkf_best_threshold2, xgb_rkf_os_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_rkf, x2_test, y2_xgb_test_rkf, xgb_rkf_prediction_probs1, xgb_rkf_prediction_probs2, \"Oversampled XGBoost Rkf\")\n",
    "\n",
    "lr_skf_best_threshold1, lr_skf_best_threshold2, lr_skf_os_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_skf, x2_test, y2_lr_test_skf, lr_skf_prediction_probs1, lr_skf_prediction_probs2, \"Oversampled Logistic Regression Skf\")\n",
    "rf_skf_best_threshold1, rf_skf_best_threshold2, rf_skf_os_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_skf, x2_test, y2_rf_test_skf, rf_skf_prediction_probs1, rf_skf_prediction_probs2, \"Oversampled Random Forrest Skf\")\n",
    "xgb_skf_best_threshold1, xgb_skf_best_threshold2, xgb_skf_os_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_skf, x2_test, y2_xgb_test_skf, xgb_skf_prediction_probs1, xgb_skf_prediction_probs2, \"Oversampled XGBoost Skf\")\n",
    "\n",
    "lr_tss_best_threshold1, lr_tss_best_threshold2, lr_tss_os_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_tss, x2_test, y2_lr_test_tss, lr_tss_prediction_probs1, lr_tss_prediction_probs2, \"Oversampled Logistic Regression Tss\")\n",
    "rf_tss_best_threshold1, rf_tss_best_threshold2, rf_tss_os_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_tss, x2_test, y2_rf_test_tss, rf_tss_prediction_probs1, rf_tss_prediction_probs2, \"Oversampled Random Forrest Tss\")\n",
    "xgb_tss_best_threshold1, xgb_tss_best_threshold2, xgb_tss_os_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_tss, x2_test, y2_xgb_test_tss, xgb_tss_prediction_probs1, xgb_tss_prediction_probs2, \"Oversampled XGBoost Tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining Oversampling with Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = graph_df[labels1]\n",
    "x2 = graph_df[labels2]\n",
    "y = graph_df[\"Bug\"]\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)\n",
    "print(\"x1 Before SMOTE:\")\n",
    "print(x1.shape)\n",
    "print(\"x2 Before SMOTE:\")\n",
    "print(x2.shape)\n",
    "\n",
    "# Fit the model to generate the data for Model 1.\n",
    "oversampled_X1, oversampled_Y1 = sm.fit_resample(x1, y)\n",
    "\n",
    "# Fit the model to generate the data for Model 2.\n",
    "oversampled_X2, oversampled_Y2 = sm.fit_resample(x2, y)\n",
    "\n",
    "print(\"x1 After SMOTE:\")\n",
    "print(oversampled_X1.shape)\n",
    "print(\"x2 After SMOTE:\")\n",
    "print(oversampled_X2.shape)\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "print(oversampled_Y1.value_counts(normalize=True) * 100)\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "print(oversampled_Y2.value_counts(normalize=True) * 100)\n",
    "\n",
    "osx1 = oversampled_X1\n",
    "osx2 = oversampled_X2\n",
    "osy1 = oversampled_Y1\n",
    "osy1 = osy1.astype('int')\n",
    "osy2 = oversampled_Y2\n",
    "osy2 = osy2.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "print(\"x1 Before RandomUnderSampler:\")\n",
    "print(osx1.shape)\n",
    "print(\"x2 Before RandomUnderSampler:\")\n",
    "print(osx2.shape)\n",
    "\n",
    "balanced_x1, balanced_y1, = rus.fit_resample(osx1, osy1)\n",
    "balanced_y1 = balanced_y1.astype('int')\n",
    "balanced_x2, balanced_y2, = rus.fit_resample(osx2, osy2)\n",
    "balanced_y2 = balanced_y2.astype('int')\n",
    "\n",
    "\n",
    "print(\"x1 After RandomUnderSampler:\")\n",
    "print(balanced_x1.shape)\n",
    "print(\"x2 After RandomUnderSampler:\")\n",
    "print(balanced_x1.shape)\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "print(balanced_y1.value_counts(normalize=True) * 100)\n",
    "\n",
    "print('\\nBalance of positive and negative classes (%):')\n",
    "print(balanced_y2.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_test, y1_train, y1_test = train_test_split(balanced_x1, balanced_y1, test_size=0.3, random_state = 5)\n",
    "y1_train = y1_train.astype('int')\n",
    "y1_test = y1_test.astype('int')\n",
    "#x1_train, y1_train = sm.fit_resample(x1_train, y1_train)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(balanced_x2, balanced_y2, test_size=0.3, random_state = 5)\n",
    "y2_train = y2_train.astype('int')\n",
    "y2_test = y2_test.astype('int')\n",
    "#x2_train, y2_train = sm.fit_resample(x2_train, y2_train)\n",
    "lr_model1.fit(x1_train, y1_train)\n",
    "lr_model2.fit(x2_train, y2_train)\n",
    "rf_model1.fit(x1_train, y1_train)\n",
    "rf_model2.fit(x2_train, y2_train)\n",
    "xgb_model1.fit(x1_train, y1_train)\n",
    "xgb_model2.fit(x2_train, y2_train)\n",
    "\n",
    "lr_predictions1 = lr_model1.predict(x1_test)\n",
    "lr_predictions2 = lr_model2.predict(x2_test)\n",
    "lr_prediction_probs1 = lr_model1.predict_proba(x1_test)\n",
    "lr_prediction_probs2 = lr_model2.predict_proba(x2_test)\n",
    "\n",
    "rf_predictions1 = rf_model1.predict(x1_test)\n",
    "rf_predictions2 = rf_model2.predict(x2_test)\n",
    "rf_prediction_probs1 = rf_model1.predict_proba(x1_test)\n",
    "rf_prediction_probs2 = rf_model2.predict_proba(x2_test)\n",
    "\n",
    "xgb_predictions1 = xgb_model1.predict(x1_test)\n",
    "xgb_predictions2 = xgb_model2.predict(x2_test)\n",
    "xgb_prediction_probs1 = xgb_model1.predict_proba(x1_test)\n",
    "xgb_prediction_probs2 = xgb_model2.predict_proba(x2_test)\n",
    "\n",
    "# Score returns the mean accuracy on the given test data and labels for the provided model.\n",
    "print(f\"Logistic regression training score for model 1: {lr_model1.score(x1_test, y1_test)}\")\n",
    "print(f\"Logistic regression training score for model 2: {lr_model2.score(x2_test, y2_test)}\")\n",
    "results_data[54][0] = \"OSUS_Combination_Logistic_Regression\"\n",
    "results_data[54][1] = lr_model1.score(x1_test, y1_test)\n",
    "results_data[54][2] = lr_model2.score(x2_test, y2_test)\n",
    "\n",
    "print(f\"Random Forrest Classification training score for model 1: {rf_model1.score(x1_test, y1_test)}\")\n",
    "print(f\"Random Forrest Classification training score for model 2: {rf_model2.score(x2_test, y2_test)}\")\n",
    "results_data[55][0] = \"OSUS_Combination_Random_Forrest\"\n",
    "results_data[55][1] = rf_model1.score(x1_test, y1_test)\n",
    "results_data[55][2] = rf_model2.score(x2_test, y2_test)\n",
    "\n",
    "print(f\"XGB Classifier training score for model 1: {xgb_model1.score(x1_test, y1_test)}\")\n",
    "print(f\"XGB Classifier training score for model 2: {xgb_model2.score(x2_test, y2_test)}\")\n",
    "results_data[56][0] = \"OSUS_Combination_XGB_Classifier\"\n",
    "results_data[56][1] = xgb_model1.score(x1_test, y1_test)\n",
    "results_data[56][2] = xgb_model2.score(x2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y1_test, lr_predictions1, lr_predictions2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "results_data[54][3] = acc1\n",
    "results_data[54][4] = acc2\n",
    "results_data[54][5] = prc_val1\n",
    "results_data[54][6] = prc_val2\n",
    "results_data[54][7] = pr_auc1\n",
    "results_data[54][8] = pr_auc2\n",
    "\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y1_test, rf_predictions1, rf_predictions2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[55][3] = acc1\n",
    "results_data[55][4] = acc2\n",
    "results_data[55][5] = prc_val1\n",
    "results_data[55][6] = prc_val2\n",
    "results_data[55][7] = pr_auc1\n",
    "results_data[55][8] = pr_auc2\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores(x1_test, x2_test, y1_test, xgb_predictions1, xgb_predictions2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[56][3] = acc1\n",
    "results_data[56][4] = acc2\n",
    "results_data[56][5] = prc_val1\n",
    "results_data[56][6] = prc_val2\n",
    "results_data[56][7] = pr_auc1\n",
    "results_data[56][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compare Precision-Recall thresholds between models for oversampling and undersampling combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_best_threshold1, lr_best_threshold2, lr_osus_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_test, x2_test, y2_test, lr_prediction_probs1, lr_prediction_probs2, \"USOS Combination Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_threshold1, rf_best_threshold2, rf_osus_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_test, x2_test, y2_test, rf_prediction_probs1, rf_prediction_probs2, \"USOS Combination Random Forrest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_threshold1, xgb_best_threshold2, xgb_osus_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_test, x2_test, y2_test, xgb_prediction_probs1, xgb_prediction_probs2, \"USOS Combination XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model1, x1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_threshold_plot(lr_model2, x2_test, y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best threshold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction_bestthresh1 = (lr_model1.predict_proba(x1_test)[:,1] >= lr_best_threshold1).astype(int)\n",
    "lr_prediction_bestthresh2 = (lr_model2.predict_proba(x2_test)[:,1] >= lr_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y1_test, lr_prediction_bestthresh1, lr_prediction_bestthresh2, lr_prediction_probs1, lr_prediction_probs2, lr_model1, lr_model2)\n",
    "results_data[57][0] = \"OSUS_Logistic_Regression_Best_Threshold\"\n",
    "results_data[57][1] = lr_model1.score(x1_test, y1_test)\n",
    "results_data[57][2] = lr_model2.score(x2_test, y2_test)\n",
    "results_data[57][3] = acc1\n",
    "results_data[57][4] = acc2\n",
    "results_data[57][5] = prc_val1\n",
    "results_data[57][6] = prc_val2\n",
    "results_data[57][7] = pr_auc1\n",
    "results_data[57][8] = pr_auc2\n",
    "\n",
    "rf_prediction_bestthresh1 = (rf_model1.predict_proba(x1_test)[:,1] >= rf_best_threshold1).astype(int)\n",
    "rf_prediction_bestthresh2 = (rf_model2.predict_proba(x2_test)[:,1] >= rf_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y1_test, rf_prediction_bestthresh1, rf_prediction_bestthresh2, rf_prediction_probs1, rf_prediction_probs2, rf_model1, rf_model2)\n",
    "results_data[58][0] = \"OSUS_Random_Forrest_Best_Threshold\"\n",
    "results_data[58][1] = rf_model1.score(x1_test, y1_test)\n",
    "results_data[58][2] = rf_model2.score(x2_test, y2_test)\n",
    "results_data[58][3] = acc1\n",
    "results_data[58][4] = acc2\n",
    "results_data[58][5] = prc_val1\n",
    "results_data[58][6] = prc_val2\n",
    "results_data[58][7] = pr_auc1\n",
    "results_data[58][8] = pr_auc2\n",
    "\n",
    "xgb_prediction_bestthresh1 = (xgb_model1.predict_proba(x1_test)[:,1] >= xgb_best_threshold1).astype(int)\n",
    "xgb_prediction_bestthresh2 = (xgb_model2.predict_proba(x2_test)[:,1] >= xgb_best_threshold2).astype(int)\n",
    "acc1, acc2, prc_val1, prc_val2, pr_auc1, pr_auc2 = Compare_Model_Scores_Best_Threshold(x1_test, x2_test, y1_test, xgb_prediction_bestthresh1, xgb_prediction_bestthresh2, xgb_prediction_probs1, xgb_prediction_probs2, xgb_model1, xgb_model2)\n",
    "results_data[59][0] = \"OSUS_XGBoost_Classifier_Best_Threshold\"\n",
    "results_data[59][1] = xgb_model1.score(x1_test, y1_test)\n",
    "results_data[59][2] = xgb_model2.score(x2_test, y2_test)\n",
    "results_data[59][3] = acc1\n",
    "results_data[59][4] = acc2\n",
    "results_data[59][5] = prc_val1\n",
    "results_data[59][6] = prc_val2\n",
    "results_data[59][7] = pr_auc1\n",
    "results_data[59][8] = pr_auc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation After Oversampling/Undersampling Combination Rebalance for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf, pr_auc, lr_rkf_prediction_probs1 = Rkf(lr_model1, balanced_x1, balanced_y1)\n",
    "results_data[60][0] = \"OSUS_Combination_Logistic_Regression_rkf\"\n",
    "results_data[60][1] = model_score\n",
    "results_data[60][3] = acc\n",
    "results_data[60][5] = prc_val\n",
    "results_data[60][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs1 = Rkf(lr_model1, balanced_x1, balanced_y1, lr_best_threshold1)\n",
    "results_data[61][0] = \"OSUS_Combination_Logistic_Regression_rkf_Best_Threshold\"\n",
    "results_data[61][1] = model_score\n",
    "results_data[61][3] = acc\n",
    "results_data[61][5] = prc_val\n",
    "results_data[61][7] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model1, balanced_x1, balanced_y1)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_rkf, pr_auc, rf_rkf_prediction_probs1 = Rkf(rf_model1, balanced_x1, balanced_y1)\n",
    "results_data[62][0] = \"OSUS_Combination_Random_Forrest_rkf\"\n",
    "results_data[62][1] = model_score\n",
    "results_data[62][3] = acc\n",
    "results_data[62][5] = prc_val\n",
    "results_data[62][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs1 = Rkf(xgb_model1, balanced_x1, balanced_y1)\n",
    "results_data[63][0] = \"OSUS_Combination_XGB_Classifier_rkf\"\n",
    "results_data[63][1] = model_score\n",
    "results_data[63][3] = acc\n",
    "results_data[63][5] = prc_val\n",
    "results_data[63][7] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_skf, pr_auc, lr_skf_prediction_probs1 = Skf(lr_model1, balanced_x1, balanced_y1)\n",
    "results_data[64][0] = \"OSUS_Combination_Logistic_Regression_skf\"\n",
    "results_data[64][1] = model_score\n",
    "results_data[64][3] = acc\n",
    "results_data[64][5] = prc_val\n",
    "results_data[64][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_skf_best, pr_auc, lr_skf_best_prediction_probs1 = Skf(lr_model1, balanced_x1, balanced_y1, lr_best_threshold1)\n",
    "results_data[65][0] = \"OSUS_Combination_Logistic_Regression_skf_Best_Threshold\"\n",
    "results_data[65][1] = model_score\n",
    "results_data[65][3] = acc\n",
    "results_data[65][5] = prc_val\n",
    "results_data[65][7] = pr_auc\n",
    "\n",
    "Skf_short(lr_model1, balanced_x1, balanced_y1)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_skf, pr_auc, rf_skf_prediction_probs1 = Skf(rf_model1, balanced_x1, balanced_y1)\n",
    "results_data[66][0] = \"OSUS_Combination_Random_Forrest_skf\"\n",
    "results_data[66][1] = model_score\n",
    "results_data[66][3] = acc\n",
    "results_data[66][5] = prc_val\n",
    "results_data[66][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_skf, pr_auc, xgb_skf_prediction_probs1 = Skf(xgb_model1, balanced_x1, balanced_y1)\n",
    "results_data[67][0] = \"OSUS_Combination_XGB_Classifier_skf\"\n",
    "results_data[67][1] = model_score\n",
    "results_data[67][3] = acc\n",
    "results_data[67][5] = prc_val\n",
    "results_data[67][7] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss, pr_auc, lr_tss_prediction_probs1 = Tss(lr_model1, balanced_x1, balanced_y1)\n",
    "results_data[68][0] = \"OSUS_Combination_Logistic_Regression_tss\"\n",
    "results_data[68][1] = model_score\n",
    "results_data[68][3] = acc\n",
    "results_data[68][5] = prc_val\n",
    "results_data[68][7] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y1_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs1 = Tss(lr_model1, balanced_x1, balanced_y1, lr_best_threshold1)\n",
    "results_data[69][0] = \"OSUS_Combination_Logistic_Regression_tss_Best_Threshold\"\n",
    "results_data[69][1] = model_score\n",
    "results_data[69][3] = acc\n",
    "results_data[69][5] = prc_val\n",
    "results_data[69][7] = pr_auc\n",
    "\n",
    "Tss_short(lr_model1, balanced_x1, balanced_y1)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_rf_test_tss, pr_auc, rf_tss_prediction_probs1 = Skf(rf_model1, balanced_x1, balanced_y1)\n",
    "results_data[70][0] = \"OSUS_Combination_Random_Forrest_tss\"\n",
    "results_data[70][1] = model_score\n",
    "results_data[70][3] = acc\n",
    "results_data[70][5] = prc_val\n",
    "results_data[70][7] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y1_xgb_test_tss, pr_auc, xgb_tss_prediction_probs1 = Tss(xgb_model1, balanced_x1, balanced_y1)\n",
    "results_data[71][0] = \"OSUS_Combination_XGB_Classifier_tss\"\n",
    "results_data[71][1] = model_score\n",
    "results_data[71][3] = acc\n",
    "results_data[71][5] = prc_val\n",
    "results_data[71][7] = pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation After Oversampling/Undersampling Combination Rebalance for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf, pr_auc, lr_rkf_prediction_probs2 = Rkf(lr_model2, balanced_x2, balanced_y2)\n",
    "results_data[60][2] = model_score\n",
    "results_data[60][4] = acc\n",
    "results_data[60][6] = prc_val\n",
    "results_data[60][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_rkf_best, pr_auc, lr_rkf_best_prediction_probs2 = Rkf(lr_model2, balanced_x2, balanced_y2, lr_best_threshold2)\n",
    "results_data[61][2] = model_score\n",
    "results_data[61][4] = acc\n",
    "results_data[61][6] = prc_val\n",
    "results_data[61][8] = pr_auc\n",
    "\n",
    "Rkf_short(lr_model2, balanced_x2, balanced_y2)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_rkf, pr_auc, rf_rkf_prediction_probs2 = Rkf(rf_model2, balanced_x2, balanced_y2)\n",
    "results_data[62][2] = model_score\n",
    "results_data[62][4] = acc\n",
    "results_data[62][6] = prc_val\n",
    "results_data[62][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_rkf, pr_auc, xgb_rkf_prediction_probs2 = Rkf(xgb_model2, balanced_x2, balanced_y2)\n",
    "results_data[63][2] = model_score\n",
    "results_data[63][4] = acc\n",
    "results_data[63][6] = prc_val\n",
    "results_data[63][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf, pr_auc, lr_skf_prediction_probs2 = Skf(lr_model2, balanced_x2, balanced_y2)\n",
    "results_data[64][2] = model_score\n",
    "results_data[64][4] = acc\n",
    "results_data[64][6] = prc_val\n",
    "results_data[64][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_skf_best, pr_auc, lr_skf_best_prediction_probs2 = Skf(lr_model2, balanced_x2, balanced_y2, lr_best_threshold2)\n",
    "results_data[65][2] = model_score\n",
    "results_data[65][4] = acc\n",
    "results_data[65][6] = prc_val\n",
    "results_data[65][8] = pr_auc\n",
    "\n",
    "Skf_short(lr_model2, balanced_x2, balanced_y2)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_skf, pr_auc, rf_skf_prediction_probs2 = Skf(rf_model2, balanced_x2, balanced_y2)\n",
    "results_data[66][2] = model_score\n",
    "results_data[66][4] = acc\n",
    "results_data[66][6] = prc_val\n",
    "results_data[66][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_skf, pr_auc, xgb_skf_prediction_probs2 = Skf(xgb_model2, balanced_x2, balanced_y2)\n",
    "results_data[67][2] = model_score\n",
    "results_data[67][4] = acc\n",
    "results_data[67][6] = prc_val\n",
    "results_data[67][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--------------------------------\\n|Scores for Logistic Regression|\\n--------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss, pr_auc, lr_tss_prediction_probs2 = Tss(lr_model2, balanced_x2, balanced_y2)\n",
    "results_data[68][2] = model_score\n",
    "results_data[68][4] = acc\n",
    "results_data[68][6] = prc_val\n",
    "results_data[68][8] = pr_auc\n",
    "print(\"---------------------\")\n",
    "print(\"With best threshold\")\n",
    "model_score, acc, prc_val, y2_lr_test_tss_best, pr_auc, lr_tss_best_prediction_probs2 = Tss(lr_model2, balanced_x2, balanced_y2, lr_best_threshold2)\n",
    "results_data[69][2] = model_score\n",
    "results_data[69][4] = acc\n",
    "results_data[69][6] = prc_val\n",
    "results_data[69][8] = pr_auc\n",
    "\n",
    "Tss_short(lr_model2, balanced_x2, balanced_y2)\n",
    "print(\"--------------------------------------\\n|Scores for Random Forrest Classifier|\\n--------------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_rf_test_tss, pr_auc, rf_tss_prediction_probs2 = Skf(rf_model2, balanced_x2, balanced_y2)\n",
    "results_data[70][2] = model_score\n",
    "results_data[70][4] = acc\n",
    "results_data[70][6] = prc_val\n",
    "results_data[70][8] = pr_auc\n",
    "\n",
    "print(\"-------------------------------\\n|Scores for XGBoost Classifier|\\n-------------------------------\")\n",
    "print(\"With normal threshold\")\n",
    "model_score, acc, prc_val, y2_xgb_test_tss, pr_auc, xgb_tss_prediction_probs2 = Tss(xgb_model2, balanced_x2, balanced_y2)\n",
    "results_data[71][2] = model_score\n",
    "results_data[71][4] = acc\n",
    "results_data[71][6] = prc_val\n",
    "results_data[71][8] = pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_rkf_best_threshold1, lr_rkf_best_threshold2, lr_rkf_osus_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_rkf, x2_test, y2_lr_test_rkf, lr_rkf_prediction_probs1, lr_rkf_prediction_probs2, \"OSUS Logistic Regression Rkf\")\n",
    "#rf_rkf_best_threshold1, rf_rkf_best_threshold2, rf_rkf_osus_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_rkf, x2_test, y2_rf_test_rkf, rf_rkf_prediction_probs1, rf_rkf_prediction_probs2, \"OSUS Random Forrest Rkf\")\n",
    "#xgb_rkf_best_threshold1, xgb_rkf_best_threshold2, xgb_rkf_osus_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_rkf, x2_test, y2_xgb_test_rkf, xgb_rkf_prediction_probs1, xgb_rkf_prediction_probs2, \"OSUS XGBoost Rkf\")\n",
    "\n",
    "lr_skf_best_threshold1, lr_skf_best_threshold2, lr_skf_osus_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_skf, x2_test, y2_lr_test_skf, lr_skf_prediction_probs1, lr_skf_prediction_probs2, \"OSUS Logistic Regression Skf\")\n",
    "rf_skf_best_threshold1, rf_skf_best_threshold2, rf_skf_osus_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_skf, x2_test, y2_rf_test_skf, rf_skf_prediction_probs1, rf_skf_prediction_probs2, \"OSUS Random Forrest Skf\")\n",
    "xgb_skf_best_threshold1, xgb_skf_best_threshold2, xgb_skf_osus_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_skf, x2_test, y2_xgb_test_skf, xgb_skf_prediction_probs1, xgb_skf_prediction_probs2, \"OSUS XGBoost Skf\")\n",
    "\n",
    "lr_tss_best_threshold1, lr_tss_best_threshold2, lr_tss_osus_fig = plot_thresholds(lr_model1, lr_model2, x1_test, y1_lr_test_tss, x2_test, y2_lr_test_tss, lr_tss_prediction_probs1, lr_tss_prediction_probs2, \"OSUS Logistic Regression Tss\")\n",
    "rf_tss_best_threshold1, rf_tss_best_threshold2, rf_tss_osus_fig = plot_thresholds(rf_model1, rf_model2, x1_test, y1_rf_test_tss, x2_test, y2_rf_test_tss, rf_tss_prediction_probs1, rf_tss_prediction_probs2, \"OSUS Random Forrest Tss\")\n",
    "xgb_tss_best_threshold1, xgb_tss_best_threshold2, xgb_tss_osus_fig = plot_thresholds(xgb_model1, xgb_model2, x1_test, y1_xgb_test_tss, x2_test, y2_xgb_test_tss, xgb_tss_prediction_probs1, xgb_tss_prediction_probs2, \"OSUS XGBoost Tss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_data, columns = ['Test', 'Model1 score', 'Model2 score', 'Model1 accuracy', 'Model2 accuracy', 'Model1 avg. PR score', 'Model2 avg. PR score', 'Model1 PRC-AUC Score', 'Model2 PRC-AUC Score'])\n",
    "model1_results_df = results_df[['Test', 'Model1 score', 'Model1 accuracy', 'Model1 avg. PR score', 'Model1 PRC-AUC Score']]\n",
    "model2_results_df = results_df[['Test', 'Model2 score', 'Model2 accuracy', 'Model2 avg. PR score', 'Model2 PRC-AUC Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_results_df = model1_results_df[model1_results_df['Model1 PRC-AUC Score'].notna()]\n",
    "model1_results_df[~model1_results_df.Test.str.contains(\"tss\", na=False)].sort_values(by=['Model1 PRC-AUC Score', 'Model1 score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_results_df = model2_results_df[model2_results_df['Model2 PRC-AUC Score'].notna()]\n",
    "model2_results_df[~model2_results_df.Test.str.contains(\"tss\", na=False)].sort_values(by=['Model2 PRC-AUC Score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures = [lr_osus_fig, rf_osus_fig, xgb_os_fig]\n",
    "\n",
    "for i, figure in enumerate(figures):\n",
    "    figure.savefig(f\"../img/ActiveMQ/graphs/Precision vs. Recall Results/Figure_{i}\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_A = mpimg.imread('../img/ActiveMQ/graphs/Precision vs. Recall Results/Figure_0.png')\n",
    "img_B = mpimg.imread('../img/ActiveMQ/graphs/Precision vs. Recall Results/Figure_1.png')\n",
    "img_C = mpimg.imread('../img/ActiveMQ/graphs/Precision vs. Recall Results/Figure_2.png')\n",
    "# display images\n",
    "fig, ax = plt.subplots(1,3,figsize=(20,20))\n",
    "ax[0].imshow(img_A);\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(img_B);\n",
    "ax[1].axis('off')\n",
    "ax[2].imshow(img_C);\n",
    "ax[2].axis('off')\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"../img/Jit-Reliability/graphs/Precision vs. Recall Results/Figure_final.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
